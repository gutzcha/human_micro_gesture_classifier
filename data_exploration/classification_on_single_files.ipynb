{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:04:06.490779100Z",
     "start_time": "2024-04-17T13:04:02.859654300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\modeling_finetune.py:306: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\modeling_finetune.py:315: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\modeling_finetune.py:324: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\modeling_finetune.py:333: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\modeling_finetune.py:342: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained=False, **kwargs):\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# from modeling_pretrain import PretrainVisionTransformerMultiOutout\n",
    "from run_class_finetuning import get_args as args_parser\n",
    "from run_videomae_vis_v2 import DataAugmentationForVideoMAEInference, get_model, load_frames, save_video\n",
    "import yaml\n",
    "import os.path as osp\n",
    "import os\n",
    "from types import SimpleNamespace as Namespace\n",
    "from typing import List, Union\n",
    "from timm.models import create_model\n",
    "import torch\n",
    "from utils import load_state_dict, time_function_decorator\n",
    "from torch import sigmoid as logit\n",
    "from mpigroup.const import LABELS as LABELS_MAP\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'Adjusting_clothing',\n 1: 'Fold_arms',\n 2: 'Fumble',\n 3: 'Gesture',\n 4: 'Groom',\n 5: 'Hand_face',\n 6: 'Hand_mouth',\n 7: 'Leg_movement',\n 8: 'Legs_crossed',\n 9: 'Scratch',\n 10: 'Settle',\n 11: 'Shrug',\n 12: 'Smearing_hands',\n 13: 'Stretching'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_MAP"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:04:06.507786900Z",
     "start_time": "2024-04-17T13:04:06.494779900Z"
    }
   },
   "id": "fd7245a79276442"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'..\\\\model_configs\\\\mpigroup_multiclass_inference_debug.yaml'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths and consts\n",
    "config_path = osp.join('..','model_configs','mpigroup_multiclass_inference_debug.yaml')\n",
    "config_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:04:09.009645300Z",
     "start_time": "2024-04-17T13:04:08.984637600Z"
    }
   },
   "id": "2766722f22beb60"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def pars_path(p: Union[str, List[Union[List, str]]]):\n",
    "    \n",
    "    assert isinstance(p, (list, str)), TypeError(\"p must be a List or a str\")\n",
    "    \n",
    "    # If p is a string, return it\n",
    "    if isinstance(p, str):\n",
    "        return p\n",
    "    \n",
    "    # If p is an empty list, return an empty string\n",
    "    if len(p) == 0:\n",
    "        return ''\n",
    "    \n",
    "    # Initialize an empty list to store components of the path\n",
    "    components = []\n",
    "    \n",
    "    # Iterate over elements of the nested list\n",
    "    for item in p:\n",
    "        # Recursively call pars_path if item is a list\n",
    "        if isinstance(item, list):\n",
    "            components.append(pars_path(item))\n",
    "        # Append the string directly to components if item is a string\n",
    "        elif isinstance(item, str):\n",
    "            components.append(item)\n",
    "        else:\n",
    "            print(type(item))\n",
    "            raise TypeError(\"Invalid type in nested list\")\n",
    "    \n",
    "    # Use os.path.join() to construct the path\n",
    "    return os.path.join(*components)       \n",
    "def get_args(yaml_path):            \n",
    "    # load yaml\n",
    "    loaded_config = yaml.safe_load(open(yaml_path, 'r'))\n",
    "    finetuning_params = loaded_config['finetuning_params']\n",
    "    \n",
    "    for k, v in finetuning_params.items():\n",
    "        if isinstance(v, list):\n",
    "            if isinstance(v[0], float):\n",
    "                continue\n",
    "            v = pars_path(v)\n",
    "        finetuning_params[k] = v \n",
    "    \n",
    "    return Namespace(**finetuning_params)\n",
    "args = get_args(config_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:04:10.436863500Z",
     "start_time": "2024-04-17T13:04:10.422866200Z"
    }
   },
   "id": "1567fb318798cac3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# load model\n",
    "model = create_model(\n",
    "    args.model,\n",
    "    pretrained=False,\n",
    "    num_classes=args.nb_classes,\n",
    "    all_frames=args.num_frames * args.num_segments,\n",
    "    tubelet_size=args.tubelet_size,\n",
    "    fc_drop_rate=args.fc_drop_rate,\n",
    "    drop_rate=args.drop,\n",
    "    drop_path_rate=args.drop_path,\n",
    "    attn_drop_rate=args.attn_drop_rate,\n",
    "    drop_block_rate=None,\n",
    "    use_checkpoint=args.use_checkpoint,\n",
    "    use_mean_pooling=args.use_mean_pooling,\n",
    "    init_scale=args.init_scale,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:04:14.605337400Z",
     "start_time": "2024-04-17T13:04:11.949350600Z"
    }
   },
   "id": "84a4f0928a323f12"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "VisionTransformer(\n  (patch_embed): PatchEmbed(\n    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (blocks): ModuleList(\n    (0): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (1): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.00909090880304575)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (2): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.0181818176060915)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (3): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.027272727340459824)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (4): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.036363635212183)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (5): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.045454543083906174)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (6): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.054545458406209946)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (7): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.06363636255264282)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (8): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.0727272778749466)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (9): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.08181818574666977)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (10): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.09090909361839294)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n    (11): Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=False)\n        (attn_drop): Dropout(p=0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (drop_path): DropPath(p=0.10000000149011612)\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop): Dropout(p=0.0, inplace=False)\n      )\n    )\n  )\n  (norm): Identity()\n  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  (fc_dropout): Identity()\n  (head): Linear(in_features=768, out_features=14, bias=True)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load checkpoint\n",
    "device = torch.device(args.device)\n",
    "# p = \"D:\\\\Project-mpg microgesture\\\\human_micro_gesture_classifier\\\\scripts\\\\MPIIGroupInteraction\\\\videomae_vit_base_patch16_224_kinetic_400_densepose_dual\\\\outputs\\\\checkpoint-best\\\\mp_rank_00_model_states.pt\"\n",
    "checkpoint = torch.load(args.finetune, map_location='cpu')\n",
    "# checkpoint = torch.load(p, map_location='cpu')\n",
    "checkpoint_model = checkpoint['module']\n",
    "load_state_dict(model, checkpoint_model)\n",
    "# model_gpu = model.to(device)\n",
    "# model_cpu = model.to('cpu')\n",
    "model.eval()\n",
    "# model_gpu.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:04:32.530793400Z",
     "start_time": "2024-04-17T13:04:31.944805Z"
    }
   },
   "id": "69e7c1fbce80fba0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size = (16, 16)\n"
     ]
    }
   ],
   "source": [
    "patch_size = model.patch_embed.patch_size\n",
    "print(\"Patch size = %s\" % str(patch_size))\n",
    "args.window_size = (args.num_frames // 2, args.input_size // patch_size[0], args.input_size // patch_size[1])\n",
    "args.patch_size = patch_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:04:37.038613200Z",
     "start_time": "2024-04-17T13:04:37.018604400Z"
    }
   },
   "id": "a481196e465523f3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# load video\n",
    "\n",
    "# path_to_video = \"D:\\\\Project-mpg microgesture\\\\human_micro_gesture_classifier\\\\video_samples_results\\\\MPIG_densepose_dual_2\\\\checkpoint-99\\\\MPIIGroupInteraction\\clips_train\\\\00000-video\\\\videos\\\\ori_vid.mp4\"\n",
    "\n",
    "path_to_video = \"D:\\\\Project-mpg microgesture\\\\imigue_rgb_phase1\\\\iMiGUE_RGB_Phase1\\\\imigue_rgb_train\\\\0012\\\\0012.mp4\"\n",
    "\n",
    "transforms = DataAugmentationForVideoMAEInference(args)\n",
    "# vid = load_frames(img_path=path_to_video,num_frames=16, transformations=transforms, frame_id_list=range(16))[0]\n",
    "vid = load_frames(img_path=path_to_video,num_frames=16, transformations=transforms, frame_id_list=None)[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:05:49.975681900Z",
     "start_time": "2024-04-17T13:05:49.195681900Z"
    }
   },
   "id": "c24397c5740491ac"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def transform_video(video_data, transformations):\n",
    "    n_frames = video_data.shape[0]\n",
    "    img = [Image.fromarray(video_data[vid, :, :, :]).convert('RGB') for vid, _ in enumerate(n_frames)]\n",
    "    # Performe transformations on the image - resizeing, normalization, reshape\n",
    "    img, _ = transformations((img, None))  # T*C,H,W\n",
    "    img = img.view((n_frames, 3) + img.size()[-2:]).transpose(0, 1)  # T*C,H,W -> T,C,H,W -> C,T,H,W\n",
    "    img = img.unsqueeze(0)\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:05:52.749145100Z",
     "start_time": "2024-04-17T13:05:52.730153100Z"
    }
   },
   "id": "f62d2da56648ce55"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# run inference\n",
    "@time_function_decorator\n",
    "def run_inference(model, vid, device='cpu'):\n",
    "    vid = vid.to(device)\n",
    "    out = model(vid)\n",
    "    logits = logit(out).detach().cpu().tolist()\n",
    "    df = pd.DataFrame(logits, columns=LABELS_MAP.values()).transpose()\n",
    "    return df "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:05:57.481055500Z",
     "start_time": "2024-04-17T13:05:57.454049Z"
    }
   },
   "id": "7f2edad72be1fded"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\micro\\lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function run_inference took: 2.3477602005004883 seconds to run\n"
     ]
    },
    {
     "data": {
      "text/plain": "                           0\nAdjusting_clothing  0.836331\nFold_arms           0.016100\nFumble              0.024937\nGesture             0.129400\nGroom               0.178861\nHand_face           0.017553\nHand_mouth          0.012834\nLeg_movement        0.499612\nLegs_crossed        0.647438\nScratch             0.019225\nSettle              0.045194\nShrug               0.000594\nSmearing_hands      0.063358\nStretching          0.002992",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Adjusting_clothing</th>\n      <td>0.836331</td>\n    </tr>\n    <tr>\n      <th>Fold_arms</th>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <th>Fumble</th>\n      <td>0.024937</td>\n    </tr>\n    <tr>\n      <th>Gesture</th>\n      <td>0.129400</td>\n    </tr>\n    <tr>\n      <th>Groom</th>\n      <td>0.178861</td>\n    </tr>\n    <tr>\n      <th>Hand_face</th>\n      <td>0.017553</td>\n    </tr>\n    <tr>\n      <th>Hand_mouth</th>\n      <td>0.012834</td>\n    </tr>\n    <tr>\n      <th>Leg_movement</th>\n      <td>0.499612</td>\n    </tr>\n    <tr>\n      <th>Legs_crossed</th>\n      <td>0.647438</td>\n    </tr>\n    <tr>\n      <th>Scratch</th>\n      <td>0.019225</td>\n    </tr>\n    <tr>\n      <th>Settle</th>\n      <td>0.045194</td>\n    </tr>\n    <tr>\n      <th>Shrug</th>\n      <td>0.000594</td>\n    </tr>\n    <tr>\n      <th>Smearing_hands</th>\n      <td>0.063358</td>\n    </tr>\n    <tr>\n      <th>Stretching</th>\n      <td>0.002992</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = run_inference(model=model, vid=vid, device='cpu')\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T13:06:05.316899500Z",
     "start_time": "2024-04-17T13:06:02.946182400Z"
    }
   },
   "id": "591071de2eedee3f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# visualize results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-16T09:25:26.650515600Z",
     "start_time": "2024-04-16T09:25:26.644169700Z"
    }
   },
   "id": "60b723036497c8aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a0945a59cf9a4de8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
