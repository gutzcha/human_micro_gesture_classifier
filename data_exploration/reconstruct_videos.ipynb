{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:09:57.111735Z",
     "start_time": "2023-11-21T15:09:56.988441Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())\n",
    "\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "# from transformers import VideoMAEImageProcessor, VideoMAEForPreTraining\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from einops import rearrange\n",
    "from modeling_pretrain import (\n",
    "    pretrain_videomae_small_patch16_224,\n",
    "    pretrain_videomae_base_patch16_224,\n",
    "    pretrain_videomae_huge_patch16_224\n",
    ")\n",
    "\n",
    "import glob\n",
    "from types import SimpleNamespace\n",
    "from datasets import build_dataset\n",
    "import video_transforms as video_transforms\n",
    "import volume_transforms as volume_transforms\n",
    "\n",
    "IMG_STD = [0.229, 0.224, 0.225]\n",
    "IMG_MEAN = [0.485, 0.456, 0.406]\n",
    "\n",
    "\n",
    "def pad_frames(vid, out_len, method='reflect'):\n",
    "    len_vid = len(vid)\n",
    "    if out_len < len_vid:\n",
    "        return vid\n",
    "    ret = []\n",
    "    if method == 'reflect':\n",
    "        pad_size = out_len - len_vid\n",
    "        pad_left = pad_size // 2\n",
    "        pad_right = pad_size - pad_left\n",
    "        for il in range(pad_right):\n",
    "            ret.append(vid[0])\n",
    "        ret += vid\n",
    "        for ir in range(pad_right):\n",
    "            ret.append(vid[-1])\n",
    "    return ret\n",
    "\n",
    "\n",
    "num_frames = 16\n",
    "\n",
    "\n",
    "def load_video(video_path, num_frames=16):\n",
    "    # Load the video\n",
    "    # video, audio, info = read_video(video_path, output_format=\"TCHW\")\n",
    "    video, audio, info = read_video(video_path, output_format=\"THWC\")\n",
    "    video_shape = video.shape\n",
    "\n",
    "    # Apply the transformation to each frame in the video\n",
    "    # frames_resized = [transform(frame) for frame in video]\n",
    "    # frames_permuted = [frame.permute(1, 2, 0) for frame in frames_resized]\n",
    "    # frames_permuted = [frame.permute(1, 2, 0) for frame in video]\n",
    "    frames_permuted = video\n",
    "    frames_permuted = pad_frames(frames_permuted, num_frames, method='reflect')\n",
    "\n",
    "    return frames_permuted\n",
    "\n",
    "\n",
    "# def load_model_hf(model_name='MCG-NJU/videomae-base-short-ssv2'):\n",
    "#     feature_extractor = VideoMAEImageProcessor.from_pretrained(model_name)\n",
    "#     model = VideoMAEForPreTraining.from_pretrained(model_name)\n",
    "#     return model, feature_extractor\n",
    "\n",
    "\n",
    "# def run_model_hf(pixel_values, model, p=0.5):\n",
    "#     bool_masked_pos = get_mask(model, p)\n",
    "#     outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n",
    "#     return outputs, pixel_values\n",
    "\n",
    "\n",
    "def get_mask(p=0.5, method='random', config=None):\n",
    "    if p == 0:\n",
    "        return None\n",
    "\n",
    "    if config is None:\n",
    "        img_size = 224\n",
    "        patch_size = 16\n",
    "    else:\n",
    "        img_size = config.input_size\n",
    "        patch_size = config.patch_size[0]\n",
    "\n",
    "    tubelet_size = 2\n",
    "    num_patches_per_frame = (img_size // patch_size) ** 2\n",
    "    seq_length = (num_frames // tubelet_size) * num_patches_per_frame\n",
    "    if method == 'random':\n",
    "        return torch.rand(1, seq_length) < p\n",
    "    else:\n",
    "        num_true = int(p * 14)\n",
    "        mask = torch.zeros(1, seq_length, dtype=torch.bool)\n",
    "        mask = rearrange(mask, 'b (t h w) -> b t h w', t=8, h=14, w=14)\n",
    "        if method == 'left':\n",
    "            mask[:, :, :, :num_true] = True\n",
    "        elif method == 'right':\n",
    "            mask[:, :, :, -num_true:] = True\n",
    "        elif method == \"top\":\n",
    "            mask[:, :, :num_true, :] = True\n",
    "        elif method == 'bottom':\n",
    "            mask[:, :, -num_true:, :] = True\n",
    "        elif method == 'horizontal':\n",
    "            mask[:, :, ::2, :] = True\n",
    "        elif method == 'vertical':\n",
    "            mask[:, :, :, ::2] = True\n",
    "        elif method == 'last':\n",
    "            mask[:, -1, :, :] = True\n",
    "        elif method == 'mid':\n",
    "            inds = [a for a in range(14) if a not in [7]]\n",
    "            mask[:, 3, :, inds] = True\n",
    "        elif method == 'grid':\n",
    "            mask[:, :, ::2, ::2] = True\n",
    "        elif method == 'inv_grid':\n",
    "            mask[:, :, :, :] = True\n",
    "            mask[:, :, ::2, ::2] = False    \n",
    "        else:\n",
    "            raise f'unknown method {method}'\n",
    "\n",
    "        mask = rearrange(mask, 'b t h w -> b (t h w)', t=8, h=14, w=14)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def run_model_mine(pixel_values, model, config, p=0.9, method='random', reconstruction_mode=False):\n",
    "    if p == 0:\n",
    "        bool_masked_pos = None\n",
    "    else:\n",
    "        bool_masked_pos = get_mask(p, method, config=config)\n",
    "    outputs = model(pixel_values, mask=bool_masked_pos, reconstruction_mode=reconstruction_mode)\n",
    "    return outputs, bool_masked_pos\n",
    "\n",
    "\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def plot_video(image_list, save_path=None, override_save=False):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Function to update the plot with each frame\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "        frame_to_show = image_list[frame]\n",
    "        #\n",
    "        # ax.imshow(np.transpose(frame_to_show, [1,2,0]))\n",
    "        ax.imshow(frame_to_show)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Create an animation\n",
    "    ani = FuncAnimation(fig, update, frames=len(image_list), repeat=True)\n",
    "\n",
    "    # Save the animation\n",
    "    if save_path is not None:\n",
    "        # create folders if they don't exist\n",
    "        create_directory_if_not_exists(directory=osp.dirname(save_path))\n",
    "\n",
    "        # check if a file with the same name exists\n",
    "        if osp.exists(save_path) and (not override_save):\n",
    "            raise f'{save_path} allready exists, set override to true to override'\n",
    "\n",
    "        # get writer name\n",
    "        file_type = os.path.splitext(save_path)[-1][1:]\n",
    "        if file_type == 'gif':\n",
    "            writer = 'imagemagick'\n",
    "        elif file_type == 'mp4':\n",
    "            writer = 'ffmpeg'\n",
    "        else:\n",
    "            raise f'Save video type must gif or mp4 but it was {file_type} instead'\n",
    "        # save the file\n",
    "        print(f'Saving file {save_path}')\n",
    "        ani.save(save_path, writer=writer, fps=30)  # Adjust the fps as needed\n",
    "\n",
    "    # Display the animation as a GIF in the Jupyter Notebook\n",
    "    display(HTML(ani.to_jshtml()))\n",
    "\n",
    "    # Close the figure to avoid a double display\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def transform_video(video):\n",
    "    # pixel_values_raw = feature_extractor(video, return_tensors=\"pt\").pixel_values\n",
    "    # pixel_values = rearrange(pixel_values_raw, 'b t c h w -> b c t h w')\n",
    "\n",
    "    pixel_values_raw = data_transform(video)\n",
    "    pixel_values = rearrange(pixel_values_raw, 'F C H W -> 1 F C H W')\n",
    "\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def post_processing(outputs, mask, pixel_values, image_std, image_mean, reconstruction_mode):\n",
    "    outputs = outputs.detach().numpy()\n",
    "    try:\n",
    "        mask = mask.detach().numpy()\n",
    "    except:\n",
    "        pass\n",
    "    image_std_torch = np.array(image_std)[None, :, None, None, None]\n",
    "    image_mean_torch = np.array(image_mean)[None, :, None, None, None]\n",
    "    pixel_values_unnorm = pixel_values * image_std_torch + image_mean_torch\n",
    "    videos_patch = rearrange(pixel_values_unnorm, 'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2 c)', p0=2, p1=16,\n",
    "                             p2=16)\n",
    "    patch_size = 16\n",
    "    # output_mean = 0.48\n",
    "    # output_std = 0.08\n",
    "    # outputs_unnorm = outputs * output_std + output_mean\n",
    "    outputs_unnorm = outputs\n",
    "\n",
    "    if mask is None:\n",
    "        outputs_reconstruction = outputs_unnorm\n",
    "    else:\n",
    "        B, num_patches = mask.shape\n",
    "        _, _, patch_pix = outputs.shape  # 3 c * 16 p * 16 p\n",
    "        outputs_reconstruction = np.zeros((B, num_patches, patch_pix))\n",
    "\n",
    "        if reconstruction_mode:\n",
    "            num_mask = np.sum(~mask)\n",
    "            outputs_reconstruction[~mask, :] = outputs_unnorm[:, :num_mask, :]\n",
    "            outputs_reconstruction[mask, :] = outputs_unnorm[:, num_mask:, :]\n",
    "        else:\n",
    "            outputs_reconstruction[~mask, :] = videos_patch[~mask, :]\n",
    "            outputs_reconstruction[mask, :] = outputs_unnorm\n",
    "\n",
    "    # outputs_reconstruction[:,:] = outputs_unnorm\n",
    "   \n",
    "    video_reconstruction = rearrange(outputs_reconstruction, 'b (t h w) (p0 p1 p2 c) -> b (t p0) c (h p1) (w p2)', t=8,\n",
    "                                     h=14, w=14, p0=2, p1=patch_size, p2=patch_size)\n",
    "    video_reconstruction = video_reconstruction.squeeze()\n",
    "    video_reconstruction_transposed = video_reconstruction.transpose(0, 2, 3, 1)\n",
    "    return video_reconstruction_transposed\n",
    "\n",
    "\n",
    "def reconstruct_video(model, video, mask_prob=0.5, mask_method='bottom', reconstruction_mode=False, save_path=None,\n",
    "                      override_save=False, args=None, image_std=IMG_STD, image_mean=IMG_MEAN):\n",
    "    '''\n",
    "    Run inference on a video and recreate the video and saves it as an mp4 or gif file\n",
    "    '''\n",
    "    pixel_values = transform_video(video)\n",
    "    mask = get_mask(p=mask_prob, method=mask_method, config=args)\n",
    "    outputs = model(pixel_values, mask=mask, reconstruction_mode=reconstruction_mode)\n",
    "\n",
    "    video_reconstruction_transposed = post_processing(outputs, mask, pixel_values, image_std, image_mean,\n",
    "                                                      reconstruction_mode)\n",
    "\n",
    "    plot_video(video_reconstruction_transposed, save_path=save_path, override_save=override_save)\n",
    "\n",
    "\n",
    "def load_model(model_path, model_type=None):\n",
    "    if model_type is None:\n",
    "        model_type = pretrain_videomae_small_patch16_224\n",
    "\n",
    "    model = model_type(decoder_depth=4)\n",
    "    checkpoint = torch.load(\n",
    "        model_path, map_location=\"cpu\"\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_name_from_path(path):\n",
    "    return osp.split(path)[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd60acb138cf752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:10:04.156338Z",
     "start_time": "2023-11-21T15:09:59.631243Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_hf, feature_extractor = load_model_hf()\n",
    "# model_path = '/home/ubuntu/efs/trained_models/lsfb_isol_videomae_pretrain_small_patch16_224_frame_16x4_tube_mask_ratio_0.9_e1600/checkpoint-1364.pth'\n",
    "# model_path = '/home/ubuntu/efs/trained_models/lsfb_isol_videomae_pretrain_small_patch16_224_frame_16x4_tube_mask_ratio_0.9_e1600/checkpoint-1414.pth'\n",
    "# model_path = '/home/ubuntu/efs/trained_models/lsfb_isol_videomae_pretrain_small_patch16_224_frame_16x4_tube_mask_ratio_0.9_e1600/checkpoint-1569.pth'\n",
    "\n",
    "# model_path = '/home/ubuntu/efs/trained_models/Kinetics-400_finetune_lsfb_isol_videomae_pretrain_small_patch16_224_frame_16x4_tube_mask_ratio_0.9_e400/checkpoint-400.pth'\n",
    "# model_path = '/home/ubuntu/efs/videoMAE/pretrained/VideoMAE_ViT-S_checkpoint_Kinetics-400.pth'\n",
    "model_path = '/home/ubuntu/efs/videoMAE/pretrained/VideoMAE_ViT-B_checkpoint_Kinetics-400.pth'\n",
    "# model_path = '/home/ubuntu/efs/videoMAE/pretrained/VideoMAE _ViT-H_checkpoint_Kinetics-400.pth'\n",
    "\n",
    "# model_path = '/home/ubuntu/efs/videoMAE/pretrained/VideoMAE_ViT-S_checkpoint_ssv2.pth'\n",
    "\n",
    "\n",
    "# model = pretrain_videomae_small_patch16_224(decoder_depth=4)\n",
    "model = pretrain_videomae_base_patch16_224(decoder_depth=4)\n",
    "# model = pretrain_videomae_huge_patch16_224()\n",
    "checkpoint = torch.load(\n",
    "    model_path, map_location=\"cpu\"\n",
    ")\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3290238571d95b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:10:04.166936Z",
     "start_time": "2023-11-21T15:10:04.163350Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8507f83bcdeb267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:10:04.272659Z",
     "start_time": "2023-11-21T15:10:04.163592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\n",
    "#     model_path, map_location=\"'cuda'\"\n",
    "# )\n",
    "# model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc679e36c4cf4235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:10:04.272952Z",
     "start_time": "2023-11-21T15:10:04.272550Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_transform = video_transforms.Compose([\n",
    "                video_transforms.PadToSquare(pad_value=0),\n",
    "                video_transforms.Resize(244, interpolation='bilinear'),\n",
    "                video_transforms.CenterCrop(size=(224,224)),\n",
    "                volume_transforms.ClipToTensor(),                \n",
    "                video_transforms.Normalize(mean=IMG_MEAN,\n",
    "                                           std=IMG_STD)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd9943bf42f01b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:07:34.066741Z",
     "start_time": "2023-11-21T16:07:33.499873Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# root_folder = '/data/lsfb_dataset'\n",
    "# all_videos_paths = glob.glob(osp.join(root_folder, 'isol', 'videos', '*.mp4'))\n",
    "\n",
    "root_folder = '/videos/mpi_data/2Itzik/dyadic_communication/PIS_ID_000_SPLIT/Cam3_segmented_split/'\n",
    "all_videos_paths = glob.glob(osp.join(root_folder, '*', '*.mp4'))\n",
    "\n",
    "video_path = all_videos_paths[9]\n",
    "video = load_video(video_path)\n",
    "video = [a.numpy() for a in video]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7884c94ceb7a29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:07:56.881547Z",
     "start_time": "2023-11-21T16:07:56.338260Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2f796d7c495665",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:11:27.675682Z",
     "start_time": "2023-11-21T16:11:24.566332Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reconstruct_video(model, video, mask_prob=0.5, mask_method='inv_grid',save_path=None, reconstruction_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de9eeec07171e0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T16:11:51.276401Z",
     "start_time": "2023-11-21T16:11:47.864070Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask_prob = 0.5\n",
    "mask_method = 'horizontal'\n",
    "override_save = False\n",
    "save_path = None\n",
    "reconstruction_mode = True\n",
    "args = None\n",
    "n_frames = 16\n",
    "\n",
    "vid_len = len(video)\n",
    "n_segments = int(np.ceil(vid_len/n_frames))\n",
    "vid_len_final = int(n_segments * n_frames)\n",
    "\n",
    "padded_video = pad_frames(video, out_len=vid_len_final, method='reflect')\n",
    "pixel_values = transform_video(padded_video)\n",
    "\n",
    "pixel_values = rearrange(pixel_values, 'b  c (t s) h w -> (b s) c t h w', t=n_frames, s=n_segments)\n",
    "\n",
    "outputs = model(pixel_values, mask=None, reconstruction_mode=reconstruction_mode).detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patch_size = 16\n",
    "# Reshape the output to the original shape\n",
    "outputs_re = rearrange(outputs, 'b (t h w) (p0 p1 p2 c) -> b c (t p0) (h p1) (w p2)', t=8,\n",
    "                                     h=14, w=14, p0=2, p1=16, p2=16)\n",
    "\n",
    "# # Normalize the output using the channels \n",
    "# std_re = np.nanstd(outputs_re, axis=(0,1,3,4), keepdims=True)\n",
    "# mean_re =  np.nanmean(outputs_re, axis=(0,1,3,4), keepdims=True)\n",
    "# outputs_re  = (outputs_re - mean_re ) / std_re\n",
    "\n",
    "# Unnormalize the input\n",
    "image_std_torch = np.array(IMG_STD)[None, :, None, None, None]\n",
    "image_mean_torch = np.array(IMG_MEAN)[None, :, None, None, None]\n",
    "\n",
    "# Shift the outputs to have the same mean and std as the original image\n",
    "outputs_re_unnorm  = outputs_re * image_std_torch + image_mean_torch\n",
    "video_reconstruction_transposed = rearrange(outputs_re_unnorm, '(b s) c t h w -> 1 (b t s) h w c', t=n_frames, s=n_segments).squeeze()\n",
    "\n",
    "\n",
    "plot_video(video_reconstruction_transposed, save_path=save_path, override_save=override_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35a624868cdea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:12:45.850174Z",
     "start_time": "2023-11-21T15:12:45.480614Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_std_torch = np.array(IMG_STD)[None, :, None, None, None]\n",
    "image_mean_torch = np.array(IMG_MEAN)[None, :, None, None, None]\n",
    "pixel_values_unnorm = pixel_values * image_std_torch + image_mean_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49195e263f7926",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:12:58.855907Z",
     "start_time": "2023-11-21T15:12:58.490638Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs_re.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324639e84fbd445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:15:52.533895Z",
     "start_time": "2023-11-21T15:15:52.158459Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.std(pixel_values_unnorm, dim=(0,2,3,4), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c943cb0020a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T14:06:29.552835Z",
     "start_time": "2023-11-21T14:06:29.197218Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f30505fd9f1b1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:39:05.351163Z",
     "start_time": "2023-11-21T13:39:04.916999Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_re = np.nanmean(outputs_re,axis=[0,1,3,4] )\n",
    "std_re =  np.nanstd(outputs_re, axis=[0,1,3,4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa48d4cae82ede3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:33:57.446132Z",
     "start_time": "2023-11-21T13:33:57.411202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_mean = 0.48\n",
    "output_std = 0.08\n",
    "outputs_unnorm = outputs * output_std + output_mean\n",
    "# outputs_unnorm = (outputs - output_mean) * output_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5a663538436b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T15:16:42.628011Z",
     "start_time": "2023-11-21T15:16:42.219051Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.std(pixel_values_unnorm, dim=(0,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf47b7694ba77f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T13:00:01.647547Z",
     "start_time": "2023-11-21T13:00:01.306335Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.min(video_reconstruction_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92d6e1c843858",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruct_video(model, [a.numpy() for a in video], mask_prob=0.5, mask_method='grid',save_path=None, reconstruction_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa6e09f906b816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:16:31.308435Z",
     "start_time": "2023-11-20T15:16:21.178326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reconstruct_video(model, video, mask_prob=0.5, mask_method='random',save_path=None)\n",
    "# reconstruct_video(model, video, mask_prob=0.5, mask_method='horizontal',save_path=None, reconstruction_mode=True)\n",
    "reconstruct_video(model, [a.numpy() for a in video], mask_prob=0.99, mask_method='horizontal',save_path=None, reconstruction_mode=True)\n",
    "# reconstruct_video(model, video, mask_prob=0, mask_method='random',save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d394c6587d6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T14:49:18.394873Z",
     "start_time": "2023-11-20T14:49:18.263743Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538a329afd012c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:09:56.972316Z",
     "start_time": "2023-11-17T13:09:56.712311Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trained_models_paths_all = glob.glob('/home/ubuntu/efs/trained_models/lsfb_isol_videomae_pretrain_small_patch16_224_frame_16x4_tube_mask_ratio_0.9_e1600/*.pth')\n",
    "trained_models_checkpoint_numbers = [int(a.split('checkpoint-')[-1].split('.pth')[0]) for a in trained_models_paths_all]\n",
    "df_model_ckpt = pd.DataFrame(zip(trained_models_paths_all,trained_models_checkpoint_numbers), columns=['paths','number'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba45d353b2641809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:09:59.464003Z",
     "start_time": "2023-11-17T13:09:59.366525Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model_ckpt = df_model_ckpt.sort_values('number').reset_index(drop=True)\n",
    "inds = list(range(0,300,50))+list(range(300,len(df_model_ckpt),5))+[len(df_model_ckpt)-1]\n",
    "lsfb_paths = df_model_ckpt.iloc[inds].paths.values\n",
    "\n",
    "# trained_models_paths = df_model_ckpt.ilocp[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b4fadc2e4120",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:10:16.187401Z",
     "start_time": "2023-11-17T13:10:16.157380Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load videos, models, and create examples\n",
    "models_folder = '/home/ubuntu/efs/videoMAE/pretrained'\n",
    "model_checkpoint_paths_pretraind = ['VideoMAE_ViT-S_checkpoint_ssv2.pth','VideoMAE_ViT-S_checkpoint_Kinetics-400.pth']\n",
    "model_checkpoint_paths_pretraind = [osp.join(models_folder,p) for p in model_checkpoint_paths_pretraind]\n",
    "\n",
    "save_folder = '/home/ubuntu/efs/videoMAE/generated_videos'\n",
    "\n",
    "all_model_paths = list(lsfb_paths) + model_checkpoint_paths_pretraind\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ca7c809177c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:11:02.837684Z",
     "start_time": "2023-11-17T13:11:02.543942Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_model_names = [get_name_from_path(a) for a in all_model_paths]\n",
    "all_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a0e88cc5d501a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:10:12.422323Z",
     "start_time": "2023-11-17T13:10:12.154224Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "# # load all videos\n",
    "# videos_paths = all_videos_paths[1:5]\n",
    "# all_videos_loaded = {}\n",
    "# for video_path in videos_paths:\n",
    "#     video_name =  get_name_from_path(video_path)\n",
    "#     all_videos_loaded[video_name] = load_video(video_path)\n",
    "#     \n",
    "# \n",
    "# for model_path in all_model_paths:\n",
    "#     model = load_model(model_path)\n",
    "#     model_name = get_name_from_path(osp.split(model_path)[-1].split('.')[0])\n",
    "#     for video_name, video in all_videos_loaded.items():\n",
    "#         video_name += '_reconstructed.gif'        \n",
    "#         save_path = osp.join(save_folder, model_name, video_name)\n",
    "#         try:\n",
    "#             reconstruct_video(model, video, mask_prob=0.5, mask_method='horizontal',save_path=save_path, reconstruction_mode=True)\n",
    "#         except:\n",
    "#             print(f'Skipping file {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b22883f5b2bcf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T13:19:41.406316Z",
     "start_time": "2023-11-17T13:19:30.603968Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "def compare_gifs_in_grid_old(models_folder, gif_names, all_model_names=None):\n",
    "    if all_model_names is None:\n",
    "        models_list = os.listdir(models_folder)\n",
    "    else:\n",
    "        models_list = all_model_names\n",
    "        \n",
    "    models_list = [a for a in models_list if osp.exists(osp.join(models_folder,a))]\n",
    "        \n",
    "    num_models = len(models_list)\n",
    "    num_gifs = len(gif_names)\n",
    "\n",
    "    fig, axes = plt.subplots(num_models, num_gifs, figsize=(num_gifs * 5, num_models * 5))\n",
    "\n",
    "    for i, model_folder in enumerate(models_list):\n",
    "        model_path = os.path.join(models_folder, model_folder)\n",
    "\n",
    "        for j, gif_name in enumerate(gif_names):\n",
    "            try:\n",
    "                gif_path = os.path.join(model_path, gif_name)\n",
    "                \n",
    "                # Read the GIF using imageio\n",
    "                gif = imageio.mimread(gif_path)\n",
    "    \n",
    "                # Display the GIF in the corresponding grid cell\n",
    "                axes[i, j].imshow(gif[0])  # Display the first frame for simplicity\n",
    "                # axes[i, j].set_title(f\"{model_folder} - {gif_name}\")\n",
    "                axes[i, j].set_title(f\"{model_folder}\")\n",
    "                axes[i, j].axis('off')\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "save_folder = '/home/ubuntu/efs/videoMAE/generated_videos'\n",
    "models_folder = save_folder\n",
    "gif_names = ['CLSFBI2314A_S048_B_110920_111100_reconstructed.gif', 'CLSFBI3306A_S067_B_68737_68857_reconstructed.gif']\n",
    "compare_gifs_in_grid_old(models_folder, gif_names, all_model_names=all_model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3091b6a9f7bbbda2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:01:07.184574Z",
     "start_time": "2023-11-20T15:01:06.857976Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pixel_values[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4131e2bcce15c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:01:41.839246Z",
     "start_time": "2023-11-20T15:01:41.405831Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "from itertools import product\n",
    "\n",
    "def compare_gifs_in_grid(models_folder, gif_names, all_model_names=None, save_path=None, display_flag=False):\n",
    "    if save_path is None and not display_flag:\n",
    "        Warning(f'This function will neither save nor display the images')\n",
    "        \n",
    "    if all_model_names is None:\n",
    "        models_list = os.listdir(models_folder)\n",
    "    else:\n",
    "        models_list = all_model_names\n",
    "        \n",
    "    models_list = [a for a in models_list if os.path.exists(os.path.join(models_folder, a))]\n",
    "        \n",
    "    num_models = len(models_list)\n",
    "    num_gifs = len(gif_names)\n",
    "\n",
    "    fig, axes = plt.subplots(num_models, num_gifs, figsize=(num_gifs * 5, num_models * 5))\n",
    "\n",
    "    def update(frame):\n",
    "        for i, model_folder in enumerate(models_list):\n",
    "            model_path = os.path.join(models_folder, model_folder)\n",
    "\n",
    "            for j, gif_name in enumerate(gif_names):\n",
    "                try:\n",
    "                    gif_path = os.path.join(model_path, gif_name)\n",
    "\n",
    "                    # Read the GIF using imageio\n",
    "                    gif = imageio.mimread(gif_path)\n",
    "\n",
    "                    # Display the GIF in the corresponding grid cell\n",
    "                    axes[i, j].imshow(gif[frame])\n",
    "                    axes[i, j].set_title(f\"{model_folder}\")\n",
    "                    axes[i, j].axis('off')\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    # Get the maximum number of frames among all GIFs\n",
    "    max_frames = max(imageio.get_reader(os.path.join(models_folder, model, gif_names[0])).get_length()\n",
    "                    for model in models_list)\n",
    "\n",
    "    # Create an animation\n",
    "    ani = FuncAnimation(fig, update, frames=max_frames, repeat=True)\n",
    "\n",
    "    if display_flag:\n",
    "        # Display the animation in the Jupyter Notebook\n",
    "        display(HTML(ani.to_jshtml()))\n",
    "    if save_path is not None:\n",
    "        print(f'saving animation at {save_path}')\n",
    "        ani.save(save_path, writer='Pillow', fps=30)  # Adjust the fps as needed\n",
    "        print('Done')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return ani\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1e0447dfd1ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:05:44.269944Z",
     "start_time": "2023-11-20T15:05:43.390442Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_model_names = [\n",
    "    '/home/ubuntu/efs/trained_models/lsfb_isol_videomae_pretrain_small_patch16_224_frame_16x4_tube_mask_ratio_0.9_e1600/checkpoint-1569.pth',\n",
    "    '/home/ubuntu/efs/videoMAE/pretrained/VideoMAE_ViT-S_checkpoint_Kinetics-400.pth',\n",
    "    '/home/ubuntu/efs/trained_models/Kinetics-400_finetune_lsfb_isol_videomae_pretrain_small_patch16_224_frame_16x4_tube_mask_ratio_0.9_e400/checkpoint-400.pth'\n",
    "]\n",
    "gif_names = ['CLSFBI2314A_S048_B_110920_111100_reconstructed.gif', 'CLSFBI3306A_S067_B_68737_68857_reconstructed.gif']\n",
    "compare_gifs_in_grid(models_folder='/home/ubuntu/efs/videoMAE/generated_videos', gif_names=gif_names, all_model_names=all_model_names,save_path=None,display_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fef8d8dd0803de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-20T15:02:46.476225Z",
     "start_time": "2023-11-20T15:02:46.270435Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00695f4368156b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "save_folder = '/home/ubuntu/efs/videoMAE/generated_videos'\n",
    "models_folder = save_folder\n",
    "save_path = osp.join(models_folder,'Results.gif')\n",
    "gif_names = ['CLSFBI2314A_S048_B_110920_111100_reconstructed.gif', 'CLSFBI3306A_S067_B_68737_68857_reconstructed.gif']\n",
    "ani = compare_gifs_in_grid(models_folder, gif_names, all_model_names=all_model_names, save_path=save_path, display_flag=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
