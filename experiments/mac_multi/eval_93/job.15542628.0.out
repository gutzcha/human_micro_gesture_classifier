Not using distributed mode
Namespace(multi_labels=True, one_hot_labels=False, pos_weight_path='/lustre/fast/fast/ygoussha/mac2024_experiments/dataset/weights.json', limit_data=None, batch_size=2, epochs=300, update_freq=1, save_ckpt_freq=10, model='vit_base_patch16_224', tubelet_size=2, input_size=224, fc_drop_rate=0.0, drop=0.0, attn_drop_rate=0.0, drop_path=0.1, disable_eval_during_finetuning=False, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=[0.9, 0.999], clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.0005, layer_decay=0.75, warmup_lr=1e-06, min_lr=1e-06, warmup_epochs=5, warmup_steps=-1, color_jitter=0.4, num_sample=1, aa='rand-m7-n4-mstd0.5-inc1', smoothing=0.0, train_interpolation='bicubic', crop_pct=None, short_side_size=224, test_num_segment=1, test_num_crop=1, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=0.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='/lustre/fast/fast/ygoussha/mac2024_experiments/checkpoint-best.pth', model_key='model|module', model_prefix='', init_scale=0.001, use_checkpoint=False, use_mean_pooling=True, data_path='/lustre/fast/fast/ygoussha/mac2024_experiments/dataset', eval_data_path=None, nb_classes=56, imagenet_default_mean_and_std=True, num_segments=1, num_frames=16, sampling_rate=4, data_set='dyadic_communication_mpigroup', output_dir='/lustre/fast/fast/ygoussha/mac2024_experiments/eval_93', data_root='/lustre/fast/fast/ygoussha/mac2024/track1', log_dir='/lustre/fast/fast/ygoussha/mac2024_experiments/eval_93', device='cuda', seed=0, resume='', auto_resume=True, save_ckpt=True, start_epoch=0, eval=True, dist_eval=True, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', enable_deepspeed=False, distributed=False)
Number of the class = 56
Number of the class = 56
No test datset, trying to use val as test
Number of the class = 56
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x14d0db0f0af0>
Patch size = (16, 16)
Load ckpt from /lustre/fast/fast/ygoussha/mac2024_experiments/checkpoint-best.pth
Load state_dict by model_key = model
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.00909090880304575)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0181818176060915)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.027272727340459824)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.036363635212183)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.045454543083906174)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.054545458406209946)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.06363636255264282)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.0727272778749466)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.08181818574666977)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.09090909361839294)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath(p=0.10000000149011612)
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): Identity()
  (fc_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (fc_dropout): Identity()
  (head): Linear(in_features=768, out_features=56, bias=True)
)
number of params: 86270264
LR = 0.00000391
Batch size = 2
Update frequent = 1
Number of training examples = 11250
Number of training training per epoch = 5625
Assigned values = [0.023757264018058777, 0.03167635202407837, 0.04223513603210449, 0.056313514709472656, 0.07508468627929688, 0.1001129150390625, 0.13348388671875, 0.177978515625, 0.2373046875, 0.31640625, 0.421875, 0.5625, 0.75, 1.0]
Skip weight decay list:  {'cls_token', 'pos_embed'}
Param groups = {
  "layer_0_decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_0_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias"
    ],
    "lr_scale": 0.023757264018058777
  },
  "layer_1_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.q_bias",
      "blocks.0.attn.v_bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_1_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight"
    ],
    "lr_scale": 0.03167635202407837
  },
  "layer_2_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.q_bias",
      "blocks.1.attn.v_bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_2_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight"
    ],
    "lr_scale": 0.04223513603210449
  },
  "layer_3_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.q_bias",
      "blocks.2.attn.v_bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_3_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight"
    ],
    "lr_scale": 0.056313514709472656
  },
  "layer_4_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.q_bias",
      "blocks.3.attn.v_bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_4_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight"
    ],
    "lr_scale": 0.07508468627929688
  },
  "layer_5_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.q_bias",
      "blocks.4.attn.v_bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_5_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight"
    ],
    "lr_scale": 0.1001129150390625
  },
  "layer_6_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.q_bias",
      "blocks.5.attn.v_bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_6_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight"
    ],
    "lr_scale": 0.13348388671875
  },
  "layer_7_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.q_bias",
      "blocks.6.attn.v_bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_7_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight"
    ],
    "lr_scale": 0.177978515625
  },
  "layer_8_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.q_bias",
      "blocks.7.attn.v_bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_8_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight"
    ],
    "lr_scale": 0.2373046875
  },
  "layer_9_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.8.norm1.weight",
      "blocks.8.norm1.bias",
      "blocks.8.attn.q_bias",
      "blocks.8.attn.v_bias",
      "blocks.8.attn.proj.bias",
      "blocks.8.norm2.weight",
      "blocks.8.norm2.bias",
      "blocks.8.mlp.fc1.bias",
      "blocks.8.mlp.fc2.bias"
    ],
    "lr_scale": 0.31640625
  },
  "layer_9_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.8.attn.qkv.weight",
      "blocks.8.attn.proj.weight",
      "blocks.8.mlp.fc1.weight",
      "blocks.8.mlp.fc2.weight"
    ],
    "lr_scale": 0.31640625
  },
  "layer_10_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.9.norm1.weight",
      "blocks.9.norm1.bias",
      "blocks.9.attn.q_bias",
      "blocks.9.attn.v_bias",
      "blocks.9.attn.proj.bias",
      "blocks.9.norm2.weight",
      "blocks.9.norm2.bias",
      "blocks.9.mlp.fc1.bias",
      "blocks.9.mlp.fc2.bias"
    ],
    "lr_scale": 0.421875
  },
  "layer_10_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.9.attn.qkv.weight",
      "blocks.9.attn.proj.weight",
      "blocks.9.mlp.fc1.weight",
      "blocks.9.mlp.fc2.weight"
    ],
    "lr_scale": 0.421875
  },
  "layer_11_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.10.norm1.weight",
      "blocks.10.norm1.bias",
      "blocks.10.attn.q_bias",
      "blocks.10.attn.v_bias",
      "blocks.10.attn.proj.bias",
      "blocks.10.norm2.weight",
      "blocks.10.norm2.bias",
      "blocks.10.mlp.fc1.bias",
      "blocks.10.mlp.fc2.bias"
    ],
    "lr_scale": 0.5625
  },
  "layer_11_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.10.attn.qkv.weight",
      "blocks.10.attn.proj.weight",
      "blocks.10.mlp.fc1.weight",
      "blocks.10.mlp.fc2.weight"
    ],
    "lr_scale": 0.5625
  },
  "layer_12_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "blocks.11.norm1.weight",
      "blocks.11.norm1.bias",
      "blocks.11.attn.q_bias",
      "blocks.11.attn.v_bias",
      "blocks.11.attn.proj.bias",
      "blocks.11.norm2.weight",
      "blocks.11.norm2.bias",
      "blocks.11.mlp.fc1.bias",
      "blocks.11.mlp.fc2.bias"
    ],
    "lr_scale": 0.75
  },
  "layer_12_decay": {
    "weight_decay": 0.05,
    "params": [
      "blocks.11.attn.qkv.weight",
      "blocks.11.attn.proj.weight",
      "blocks.11.mlp.fc1.weight",
      "blocks.11.mlp.fc2.weight"
    ],
    "lr_scale": 0.75
  },
  "layer_13_no_decay": {
    "weight_decay": 0.0,
    "params": [
      "fc_norm.weight",
      "fc_norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "layer_13_decay": {
    "weight_decay": 0.05,
    "params": [
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
optimizer settings: {'lr': 3.90625e-06, 'weight_decay': 0.0, 'eps': 1e-08, 'betas': [0.9, 0.999]}
Use step level LR scheduler!
Set warmup steps = 28125
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = BCEWithLogitsLoss()
Auto resume checkpoint: 
Test:  [   0/2793]  eta: 2:52:21  loss: 0.0922 (0.0922)  acc1: 0.5000 (0.5000)  acc5: 1.0000 (1.0000)  time: 3.7026  data: 1.8871  max mem: 896
Test:  [  10/2793]  eta: 0:17:09  loss: 0.0140 (0.0385)  acc1: 0.5000 (0.4773)  acc5: 1.0000 (0.9773)  time: 0.3699  data: 0.1718  max mem: 896
Test:  [  20/2793]  eta: 0:09:47  loss: 0.0252 (0.0586)  acc1: 0.5000 (0.4881)  acc5: 1.0000 (0.9643)  time: 0.0375  data: 0.0003  max mem: 896
Test:  [  30/2793]  eta: 0:07:42  loss: 0.1002 (0.0875)  acc1: 0.5000 (0.4839)  acc5: 1.0000 (0.9355)  time: 0.0560  data: 0.0185  max mem: 896
Test:  [  40/2793]  eta: 0:06:54  loss: 0.1444 (0.1063)  acc1: 0.5000 (0.4390)  acc5: 0.7500 (0.8902)  time: 0.0860  data: 0.0484  max mem: 896
Test:  [  50/2793]  eta: 0:06:17  loss: 0.1223 (0.1014)  acc1: 0.5000 (0.4314)  acc5: 0.7500 (0.8775)  time: 0.0915  data: 0.0527  max mem: 896
Test:  [  60/2793]  eta: 0:05:54  loss: 0.0777 (0.0998)  acc1: 0.2500 (0.4016)  acc5: 0.7500 (0.8525)  time: 0.0869  data: 0.0478  max mem: 896
Test:  [  70/2793]  eta: 0:05:39  loss: 0.1080 (0.1072)  acc1: 0.2500 (0.3908)  acc5: 0.7500 (0.8415)  time: 0.0910  data: 0.0535  max mem: 896
Test:  [  80/2793]  eta: 0:05:30  loss: 0.1530 (0.1101)  acc1: 0.3333 (0.3856)  acc5: 0.8333 (0.8494)  time: 0.0973  data: 0.0608  max mem: 896
Test:  [  90/2793]  eta: 0:05:21  loss: 0.1349 (0.1112)  acc1: 0.3333 (0.3789)  acc5: 0.8333 (0.8385)  time: 0.0986  data: 0.0608  max mem: 896
Test:  [ 100/2793]  eta: 0:05:16  loss: 0.1139 (0.1176)  acc1: 0.5000 (0.3800)  acc5: 0.7500 (0.8347)  time: 0.1006  data: 0.0615  max mem: 896
Test:  [ 110/2793]  eta: 0:05:07  loss: 0.1137 (0.1167)  acc1: 0.3333 (0.3721)  acc5: 1.0000 (0.8323)  time: 0.0963  data: 0.0578  max mem: 896
Test:  [ 120/2793]  eta: 0:05:05  loss: 0.1090 (0.1174)  acc1: 0.2500 (0.3702)  acc5: 1.0000 (0.8317)  time: 0.0978  data: 0.0587  max mem: 896
Test:  [ 130/2793]  eta: 0:05:03  loss: 0.1160 (0.1188)  acc1: 0.5000 (0.3782)  acc5: 1.0000 (0.8331)  time: 0.1102  data: 0.0706  max mem: 896
Test:  [ 140/2793]  eta: 0:05:01  loss: 0.1122 (0.1183)  acc1: 0.5000 (0.3851)  acc5: 1.0000 (0.8414)  time: 0.1101  data: 0.0723  max mem: 896
Test:  [ 150/2793]  eta: 0:04:58  loss: 0.1082 (0.1183)  acc1: 0.5000 (0.3887)  acc5: 1.0000 (0.8436)  time: 0.1052  data: 0.0693  max mem: 896
Test:  [ 160/2793]  eta: 0:04:53  loss: 0.0740 (0.1179)  acc1: 0.3333 (0.3853)  acc5: 1.0000 (0.8450)  time: 0.0953  data: 0.0581  max mem: 896
Test:  [ 170/2793]  eta: 0:04:49  loss: 0.0688 (0.1185)  acc1: 0.3333 (0.3847)  acc5: 1.0000 (0.8424)  time: 0.0905  data: 0.0527  max mem: 896
Test:  [ 180/2793]  eta: 0:04:47  loss: 0.0524 (0.1173)  acc1: 0.5000 (0.3828)  acc5: 1.0000 (0.8387)  time: 0.0986  data: 0.0615  max mem: 896
Test:  [ 190/2793]  eta: 0:04:45  loss: 0.0687 (0.1167)  acc1: 0.3333 (0.3814)  acc5: 1.0000 (0.8436)  time: 0.1032  data: 0.0643  max mem: 896
Test:  [ 200/2793]  eta: 0:04:41  loss: 0.0540 (0.1121)  acc1: 0.5000 (0.3873)  acc5: 1.0000 (0.8502)  time: 0.0963  data: 0.0589  max mem: 896
Test:  [ 210/2793]  eta: 0:04:39  loss: 0.0141 (0.1076)  acc1: 0.5000 (0.3927)  acc5: 1.0000 (0.8573)  time: 0.0971  data: 0.0624  max mem: 896
Test:  [ 220/2793]  eta: 0:04:37  loss: 0.0158 (0.1053)  acc1: 0.5000 (0.3945)  acc5: 1.0000 (0.8626)  time: 0.1002  data: 0.0628  max mem: 896
Test:  [ 230/2793]  eta: 0:04:36  loss: 0.0225 (0.1032)  acc1: 0.5000 (0.3980)  acc5: 1.0000 (0.8653)  time: 0.1015  data: 0.0621  max mem: 896
Test:  [ 240/2793]  eta: 0:04:32  loss: 0.0600 (0.1043)  acc1: 0.5000 (0.3967)  acc5: 1.0000 (0.8633)  time: 0.0935  data: 0.0521  max mem: 896
Test:  [ 250/2793]  eta: 0:04:29  loss: 0.0649 (0.1039)  acc1: 0.5000 (0.3978)  acc5: 1.0000 (0.8618)  time: 0.0872  data: 0.0461  max mem: 896
Test:  [ 260/2793]  eta: 0:04:28  loss: 0.0605 (0.1032)  acc1: 0.5000 (0.4008)  acc5: 1.0000 (0.8651)  time: 0.0981  data: 0.0610  max mem: 896
Test:  [ 270/2793]  eta: 0:04:28  loss: 0.0605 (0.1037)  acc1: 0.5000 (0.4026)  acc5: 1.0000 (0.8664)  time: 0.1097  data: 0.0724  max mem: 896
Test:  [ 280/2793]  eta: 0:04:29  loss: 0.1019 (0.1053)  acc1: 0.5000 (0.4016)  acc5: 1.0000 (0.8667)  time: 0.1228  data: 0.0852  max mem: 896
Test:  [ 290/2793]  eta: 0:04:26  loss: 0.1138 (0.1062)  acc1: 0.5000 (0.3998)  acc5: 1.0000 (0.8679)  time: 0.1070  data: 0.0693  max mem: 896
Test:  [ 300/2793]  eta: 0:04:25  loss: 0.0762 (0.1056)  acc1: 0.5000 (0.3998)  acc5: 1.0000 (0.8700)  time: 0.0949  data: 0.0574  max mem: 896
Test:  [ 310/2793]  eta: 0:04:24  loss: 0.0762 (0.1070)  acc1: 0.3333 (0.3967)  acc5: 1.0000 (0.8684)  time: 0.1079  data: 0.0706  max mem: 896
Test:  [ 320/2793]  eta: 0:04:23  loss: 0.0618 (0.1059)  acc1: 0.3333 (0.3942)  acc5: 1.0000 (0.8714)  time: 0.1109  data: 0.0737  max mem: 896
Test:  [ 330/2793]  eta: 0:04:22  loss: 0.0722 (0.1074)  acc1: 0.3333 (0.3926)  acc5: 1.0000 (0.8672)  time: 0.1056  data: 0.0699  max mem: 896
Test:  [ 340/2793]  eta: 0:04:20  loss: 0.0734 (0.1069)  acc1: 0.3333 (0.3914)  acc5: 0.8000 (0.8673)  time: 0.0960  data: 0.0593  max mem: 896
Test:  [ 350/2793]  eta: 0:04:17  loss: 0.0652 (0.1063)  acc1: 0.5000 (0.3924)  acc5: 1.0000 (0.8697)  time: 0.0880  data: 0.0499  max mem: 896
Test:  [ 360/2793]  eta: 0:04:17  loss: 0.0694 (0.1051)  acc1: 0.5000 (0.3922)  acc5: 1.0000 (0.8720)  time: 0.0998  data: 0.0605  max mem: 896
Test:  [ 370/2793]  eta: 0:04:14  loss: 0.0156 (0.1031)  acc1: 0.5000 (0.3944)  acc5: 1.0000 (0.8741)  time: 0.0977  data: 0.0585  max mem: 896
Test:  [ 380/2793]  eta: 0:04:13  loss: 0.0156 (0.1014)  acc1: 0.5000 (0.3952)  acc5: 1.0000 (0.8748)  time: 0.0904  data: 0.0530  max mem: 896
Test:  [ 390/2793]  eta: 0:04:10  loss: 0.0260 (0.1007)  acc1: 0.5000 (0.3941)  acc5: 1.0000 (0.8735)  time: 0.0926  data: 0.0551  max mem: 896
Test:  [ 400/2793]  eta: 0:04:09  loss: 0.0644 (0.1016)  acc1: 0.3333 (0.3909)  acc5: 1.0000 (0.8711)  time: 0.0910  data: 0.0529  max mem: 896
Test:  [ 410/2793]  eta: 0:04:09  loss: 0.1057 (0.1019)  acc1: 0.3333 (0.3889)  acc5: 1.0000 (0.8724)  time: 0.1119  data: 0.0743  max mem: 896
Test:  [ 420/2793]  eta: 0:04:09  loss: 0.0826 (0.1027)  acc1: 0.3333 (0.3844)  acc5: 1.0000 (0.8736)  time: 0.1279  data: 0.0908  max mem: 896
Test:  [ 430/2793]  eta: 0:04:07  loss: 0.1473 (0.1040)  acc1: 0.0000 (0.3772)  acc5: 0.5000 (0.8655)  time: 0.1058  data: 0.0683  max mem: 896
Test:  [ 440/2793]  eta: 0:04:05  loss: 0.1473 (0.1052)  acc1: 0.0000 (0.3709)  acc5: 0.5000 (0.8573)  time: 0.0855  data: 0.0481  max mem: 896
Test:  [ 450/2793]  eta: 0:04:03  loss: 0.1446 (0.1061)  acc1: 0.0000 (0.3677)  acc5: 0.5000 (0.8554)  time: 0.0863  data: 0.0499  max mem: 896
Test:  [ 460/2793]  eta: 0:04:02  loss: 0.0737 (0.1053)  acc1: 0.2500 (0.3684)  acc5: 1.0000 (0.8575)  time: 0.0980  data: 0.0612  max mem: 896
Test:  [ 470/2793]  eta: 0:04:01  loss: 0.0520 (0.1047)  acc1: 0.5000 (0.3691)  acc5: 1.0000 (0.8589)  time: 0.1047  data: 0.0670  max mem: 896
Test:  [ 480/2793]  eta: 0:04:01  loss: 0.0809 (0.1055)  acc1: 0.3333 (0.3669)  acc5: 1.0000 (0.8557)  time: 0.1130  data: 0.0744  max mem: 896
Test:  [ 490/2793]  eta: 0:04:00  loss: 0.0677 (0.1046)  acc1: 0.3333 (0.3659)  acc5: 1.0000 (0.8580)  time: 0.1096  data: 0.0707  max mem: 896
Test:  [ 500/2793]  eta: 0:03:59  loss: 0.0515 (0.1042)  acc1: 0.3333 (0.3671)  acc5: 1.0000 (0.8588)  time: 0.0996  data: 0.0613  max mem: 896
Test:  [ 510/2793]  eta: 0:03:58  loss: 0.0515 (0.1041)  acc1: 0.5000 (0.3662)  acc5: 1.0000 (0.8567)  time: 0.1102  data: 0.0691  max mem: 896
Test:  [ 520/2793]  eta: 0:03:57  loss: 0.1279 (0.1058)  acc1: 0.2500 (0.3656)  acc5: 0.7500 (0.8538)  time: 0.1044  data: 0.0627  max mem: 896
Test:  [ 530/2793]  eta: 0:03:55  loss: 0.0835 (0.1049)  acc1: 0.3333 (0.3656)  acc5: 1.0000 (0.8561)  time: 0.1000  data: 0.0609  max mem: 896
Test:  [ 540/2793]  eta: 0:03:54  loss: 0.0530 (0.1052)  acc1: 0.3333 (0.3644)  acc5: 1.0000 (0.8537)  time: 0.1010  data: 0.0627  max mem: 896
Test:  [ 550/2793]  eta: 0:03:53  loss: 0.1005 (0.1055)  acc1: 0.5000 (0.3664)  acc5: 0.7500 (0.8541)  time: 0.0985  data: 0.0604  max mem: 896
Test:  [ 560/2793]  eta: 0:03:52  loss: 0.1008 (0.1062)  acc1: 0.5000 (0.3674)  acc5: 0.7500 (0.8535)  time: 0.0974  data: 0.0590  max mem: 896
Test:  [ 570/2793]  eta: 0:03:50  loss: 0.1092 (0.1065)  acc1: 0.3333 (0.3665)  acc5: 0.8333 (0.8532)  time: 0.0963  data: 0.0580  max mem: 896
Test:  [ 580/2793]  eta: 0:03:49  loss: 0.1058 (0.1066)  acc1: 0.3333 (0.3649)  acc5: 0.8333 (0.8523)  time: 0.0933  data: 0.0556  max mem: 896
Test:  [ 590/2793]  eta: 0:03:47  loss: 0.1312 (0.1075)  acc1: 0.2500 (0.3630)  acc5: 0.7500 (0.8497)  time: 0.0905  data: 0.0543  max mem: 896
Test:  [ 600/2793]  eta: 0:03:46  loss: 0.1234 (0.1076)  acc1: 0.2500 (0.3636)  acc5: 1.0000 (0.8509)  time: 0.0902  data: 0.0547  max mem: 896
Test:  [ 610/2793]  eta: 0:03:45  loss: 0.1141 (0.1079)  acc1: 0.3333 (0.3631)  acc5: 1.0000 (0.8515)  time: 0.0986  data: 0.0609  max mem: 896
Test:  [ 620/2793]  eta: 0:03:43  loss: 0.0975 (0.1076)  acc1: 0.3333 (0.3633)  acc5: 1.0000 (0.8506)  time: 0.0911  data: 0.0525  max mem: 896
Test:  [ 630/2793]  eta: 0:03:42  loss: 0.1050 (0.1090)  acc1: 0.5000 (0.3647)  acc5: 1.0000 (0.8490)  time: 0.0867  data: 0.0485  max mem: 896
Test:  [ 640/2793]  eta: 0:03:41  loss: 0.2346 (0.1109)  acc1: 0.5000 (0.3650)  acc5: 0.7500 (0.8467)  time: 0.1049  data: 0.0676  max mem: 896
Test:  [ 650/2793]  eta: 0:03:39  loss: 0.0893 (0.1099)  acc1: 0.5000 (0.3659)  acc5: 0.7500 (0.8479)  time: 0.0936  data: 0.0565  max mem: 896
Test:  [ 660/2793]  eta: 0:03:37  loss: 0.0325 (0.1091)  acc1: 0.5000 (0.3668)  acc5: 1.0000 (0.8487)  time: 0.0792  data: 0.0415  max mem: 896
Test:  [ 670/2793]  eta: 0:03:36  loss: 0.0559 (0.1092)  acc1: 0.5000 (0.3651)  acc5: 1.0000 (0.8468)  time: 0.0841  data: 0.0473  max mem: 896
Test:  [ 680/2793]  eta: 0:03:35  loss: 0.0833 (0.1089)  acc1: 0.5000 (0.3663)  acc5: 1.0000 (0.8484)  time: 0.0918  data: 0.0548  max mem: 896
Test:  [ 690/2793]  eta: 0:03:34  loss: 0.0832 (0.1085)  acc1: 0.5000 (0.3661)  acc5: 1.0000 (0.8496)  time: 0.1063  data: 0.0690  max mem: 896
Test:  [ 700/2793]  eta: 0:03:34  loss: 0.0446 (0.1077)  acc1: 0.3333 (0.3656)  acc5: 1.0000 (0.8513)  time: 0.1187  data: 0.0808  max mem: 896
Test:  [ 710/2793]  eta: 0:03:32  loss: 0.0418 (0.1071)  acc1: 0.3333 (0.3667)  acc5: 1.0000 (0.8534)  time: 0.1102  data: 0.0724  max mem: 896
Test:  [ 720/2793]  eta: 0:03:31  loss: 0.0620 (0.1067)  acc1: 0.5000 (0.3685)  acc5: 1.0000 (0.8547)  time: 0.0956  data: 0.0569  max mem: 896
Test:  [ 730/2793]  eta: 0:03:30  loss: 0.0965 (0.1071)  acc1: 0.5000 (0.3685)  acc5: 1.0000 (0.8547)  time: 0.0915  data: 0.0522  max mem: 896
Test:  [ 740/2793]  eta: 0:03:28  loss: 0.0608 (0.1061)  acc1: 0.5000 (0.3703)  acc5: 1.0000 (0.8567)  time: 0.0876  data: 0.0488  max mem: 896
Test:  [ 750/2793]  eta: 0:03:27  loss: 0.0901 (0.1067)  acc1: 0.5000 (0.3704)  acc5: 1.0000 (0.8575)  time: 0.0888  data: 0.0477  max mem: 896
Test:  [ 760/2793]  eta: 0:03:26  loss: 0.1199 (0.1064)  acc1: 0.5000 (0.3711)  acc5: 1.0000 (0.8581)  time: 0.0940  data: 0.0539  max mem: 896
Test:  [ 770/2793]  eta: 0:03:25  loss: 0.0769 (0.1061)  acc1: 0.5000 (0.3708)  acc5: 1.0000 (0.8596)  time: 0.1075  data: 0.0692  max mem: 896
Test:  [ 780/2793]  eta: 0:03:24  loss: 0.0835 (0.1062)  acc1: 0.5000 (0.3719)  acc5: 1.0000 (0.8606)  time: 0.1083  data: 0.0678  max mem: 896
Test:  [ 790/2793]  eta: 0:03:23  loss: 0.0887 (0.1059)  acc1: 0.4000 (0.3716)  acc5: 1.0000 (0.8619)  time: 0.0887  data: 0.0499  max mem: 896
Test:  [ 800/2793]  eta: 0:03:22  loss: 0.0645 (0.1059)  acc1: 0.3333 (0.3709)  acc5: 1.0000 (0.8615)  time: 0.0916  data: 0.0547  max mem: 896
Test:  [ 810/2793]  eta: 0:03:20  loss: 0.1154 (0.1064)  acc1: 0.3333 (0.3712)  acc5: 0.8333 (0.8614)  time: 0.0974  data: 0.0602  max mem: 896
Test:  [ 820/2793]  eta: 0:03:19  loss: 0.1186 (0.1066)  acc1: 0.5000 (0.3721)  acc5: 1.0000 (0.8622)  time: 0.0887  data: 0.0513  max mem: 896
Test:  [ 830/2793]  eta: 0:03:18  loss: 0.0821 (0.1063)  acc1: 0.5000 (0.3722)  acc5: 1.0000 (0.8631)  time: 0.0885  data: 0.0504  max mem: 896
Test:  [ 840/2793]  eta: 0:03:17  loss: 0.0709 (0.1061)  acc1: 0.5000 (0.3731)  acc5: 1.0000 (0.8639)  time: 0.0928  data: 0.0548  max mem: 896
Test:  [ 850/2793]  eta: 0:03:15  loss: 0.0990 (0.1065)  acc1: 0.5000 (0.3731)  acc5: 1.0000 (0.8632)  time: 0.0934  data: 0.0556  max mem: 896
Test:  [ 860/2793]  eta: 0:03:15  loss: 0.1077 (0.1062)  acc1: 0.4000 (0.3737)  acc5: 0.8333 (0.8632)  time: 0.1032  data: 0.0657  max mem: 896
Test:  [ 870/2793]  eta: 0:03:13  loss: 0.1032 (0.1071)  acc1: 0.3333 (0.3732)  acc5: 0.7500 (0.8615)  time: 0.1001  data: 0.0626  max mem: 896
Test:  [ 880/2793]  eta: 0:03:12  loss: 0.0842 (0.1071)  acc1: 0.3333 (0.3734)  acc5: 1.0000 (0.8626)  time: 0.0902  data: 0.0523  max mem: 896
Test:  [ 890/2793]  eta: 0:03:11  loss: 0.0845 (0.1070)  acc1: 0.4000 (0.3736)  acc5: 1.0000 (0.8636)  time: 0.0920  data: 0.0538  max mem: 896
Test:  [ 900/2793]  eta: 0:03:10  loss: 0.0954 (0.1070)  acc1: 0.3333 (0.3731)  acc5: 1.0000 (0.8636)  time: 0.0929  data: 0.0561  max mem: 896
Test:  [ 910/2793]  eta: 0:03:09  loss: 0.0503 (0.1064)  acc1: 0.5000 (0.3742)  acc5: 1.0000 (0.8651)  time: 0.0937  data: 0.0547  max mem: 896
Test:  [ 920/2793]  eta: 0:03:07  loss: 0.0753 (0.1067)  acc1: 0.5000 (0.3746)  acc5: 1.0000 (0.8648)  time: 0.0911  data: 0.0509  max mem: 896
Test:  [ 930/2793]  eta: 0:03:06  loss: 0.1084 (0.1065)  acc1: 0.5000 (0.3751)  acc5: 1.0000 (0.8657)  time: 0.0910  data: 0.0527  max mem: 896
Test:  [ 940/2793]  eta: 0:03:05  loss: 0.1078 (0.1068)  acc1: 0.5000 (0.3748)  acc5: 1.0000 (0.8657)  time: 0.0940  data: 0.0559  max mem: 896
Test:  [ 950/2793]  eta: 0:03:04  loss: 0.0903 (0.1067)  acc1: 0.3333 (0.3748)  acc5: 0.8333 (0.8658)  time: 0.0983  data: 0.0581  max mem: 896
Test:  [ 960/2793]  eta: 0:03:03  loss: 0.0843 (0.1066)  acc1: 0.4000 (0.3750)  acc5: 0.8333 (0.8653)  time: 0.0966  data: 0.0566  max mem: 896
Test:  [ 970/2793]  eta: 0:03:02  loss: 0.0843 (0.1064)  acc1: 0.3333 (0.3749)  acc5: 1.0000 (0.8659)  time: 0.0859  data: 0.0471  max mem: 896
Test:  [ 980/2793]  eta: 0:03:01  loss: 0.0592 (0.1063)  acc1: 0.5000 (0.3756)  acc5: 1.0000 (0.8663)  time: 0.0925  data: 0.0536  max mem: 896
Test:  [ 990/2793]  eta: 0:03:00  loss: 0.0405 (0.1060)  acc1: 0.5000 (0.3759)  acc5: 1.0000 (0.8672)  time: 0.1054  data: 0.0666  max mem: 896
Test:  [1000/2793]  eta: 0:02:59  loss: 0.0399 (0.1057)  acc1: 0.5000 (0.3767)  acc5: 1.0000 (0.8679)  time: 0.0927  data: 0.0549  max mem: 896
Test:  [1010/2793]  eta: 0:02:58  loss: 0.0657 (0.1066)  acc1: 0.5000 (0.3767)  acc5: 1.0000 (0.8670)  time: 0.0956  data: 0.0577  max mem: 896
Test:  [1020/2793]  eta: 0:02:57  loss: 0.0721 (0.1066)  acc1: 0.5000 (0.3772)  acc5: 1.0000 (0.8673)  time: 0.1076  data: 0.0695  max mem: 896
Test:  [1030/2793]  eta: 0:02:56  loss: 0.0744 (0.1064)  acc1: 0.5000 (0.3781)  acc5: 1.0000 (0.8679)  time: 0.1020  data: 0.0651  max mem: 896
Test:  [1040/2793]  eta: 0:02:55  loss: 0.0801 (0.1062)  acc1: 0.5000 (0.3782)  acc5: 1.0000 (0.8690)  time: 0.1019  data: 0.0649  max mem: 896
Test:  [1050/2793]  eta: 0:02:54  loss: 0.0748 (0.1058)  acc1: 0.5000 (0.3793)  acc5: 1.0000 (0.8702)  time: 0.1055  data: 0.0685  max mem: 896
Test:  [1060/2793]  eta: 0:02:53  loss: 0.0686 (0.1057)  acc1: 0.5000 (0.3794)  acc5: 1.0000 (0.8710)  time: 0.1020  data: 0.0635  max mem: 896
Test:  [1070/2793]  eta: 0:02:52  loss: 0.0637 (0.1052)  acc1: 0.5000 (0.3806)  acc5: 1.0000 (0.8719)  time: 0.0979  data: 0.0587  max mem: 896
Test:  [1080/2793]  eta: 0:02:51  loss: 0.0430 (0.1048)  acc1: 0.5000 (0.3813)  acc5: 1.0000 (0.8729)  time: 0.0929  data: 0.0523  max mem: 896
Test:  [1090/2793]  eta: 0:02:49  loss: 0.0622 (0.1047)  acc1: 0.5000 (0.3814)  acc5: 1.0000 (0.8727)  time: 0.0876  data: 0.0470  max mem: 896
Test:  [1100/2793]  eta: 0:02:48  loss: 0.0733 (0.1047)  acc1: 0.5000 (0.3815)  acc5: 1.0000 (0.8732)  time: 0.0888  data: 0.0499  max mem: 896
Test:  [1110/2793]  eta: 0:02:47  loss: 0.1336 (0.1053)  acc1: 0.5000 (0.3807)  acc5: 1.0000 (0.8720)  time: 0.0896  data: 0.0513  max mem: 896
Test:  [1120/2793]  eta: 0:02:46  loss: 0.1439 (0.1055)  acc1: 0.2500 (0.3795)  acc5: 0.7500 (0.8704)  time: 0.0850  data: 0.0473  max mem: 896
Test:  [1130/2793]  eta: 0:02:45  loss: 0.1373 (0.1058)  acc1: 0.2500 (0.3779)  acc5: 0.7500 (0.8694)  time: 0.0839  data: 0.0443  max mem: 896
Test:  [1140/2793]  eta: 0:02:43  loss: 0.1103 (0.1058)  acc1: 0.2500 (0.3781)  acc5: 0.7500 (0.8690)  time: 0.0837  data: 0.0435  max mem: 896
Test:  [1150/2793]  eta: 0:02:43  loss: 0.1130 (0.1061)  acc1: 0.3333 (0.3780)  acc5: 1.0000 (0.8695)  time: 0.0918  data: 0.0518  max mem: 896
Test:  [1160/2793]  eta: 0:02:41  loss: 0.1130 (0.1059)  acc1: 0.3333 (0.3781)  acc5: 1.0000 (0.8700)  time: 0.0889  data: 0.0464  max mem: 896
Test:  [1170/2793]  eta: 0:02:40  loss: 0.1319 (0.1072)  acc1: 0.3333 (0.3770)  acc5: 1.0000 (0.8691)  time: 0.0793  data: 0.0374  max mem: 896
Test:  [1180/2793]  eta: 0:02:39  loss: 0.1184 (0.1071)  acc1: 0.2500 (0.3768)  acc5: 0.8333 (0.8693)  time: 0.0799  data: 0.0392  max mem: 896
Test:  [1190/2793]  eta: 0:02:38  loss: 0.0934 (0.1071)  acc1: 0.5000 (0.3774)  acc5: 1.0000 (0.8694)  time: 0.0903  data: 0.0499  max mem: 896
Test:  [1200/2793]  eta: 0:02:37  loss: 0.1527 (0.1077)  acc1: 0.3333 (0.3769)  acc5: 0.8333 (0.8690)  time: 0.0950  data: 0.0567  max mem: 896
Test:  [1210/2793]  eta: 0:02:36  loss: 0.1340 (0.1076)  acc1: 0.3333 (0.3768)  acc5: 0.8333 (0.8693)  time: 0.0909  data: 0.0508  max mem: 896
Test:  [1220/2793]  eta: 0:02:34  loss: 0.0907 (0.1079)  acc1: 0.3333 (0.3764)  acc5: 1.0000 (0.8689)  time: 0.0851  data: 0.0449  max mem: 896
Test:  [1230/2793]  eta: 0:02:33  loss: 0.0858 (0.1078)  acc1: 0.5000 (0.3769)  acc5: 1.0000 (0.8696)  time: 0.0809  data: 0.0439  max mem: 896
Test:  [1240/2793]  eta: 0:02:32  loss: 0.1023 (0.1079)  acc1: 0.5000 (0.3768)  acc5: 1.0000 (0.8693)  time: 0.0955  data: 0.0580  max mem: 896
Test:  [1250/2793]  eta: 0:02:31  loss: 0.0809 (0.1079)  acc1: 0.3333 (0.3766)  acc5: 0.8333 (0.8688)  time: 0.0934  data: 0.0560  max mem: 896
Test:  [1260/2793]  eta: 0:02:30  loss: 0.0903 (0.1087)  acc1: 0.2500 (0.3758)  acc5: 0.8333 (0.8676)  time: 0.0840  data: 0.0465  max mem: 896
Test:  [1270/2793]  eta: 0:02:29  loss: 0.1058 (0.1086)  acc1: 0.2500 (0.3757)  acc5: 1.0000 (0.8683)  time: 0.1060  data: 0.0649  max mem: 896
Test:  [1280/2793]  eta: 0:02:28  loss: 0.0693 (0.1082)  acc1: 0.5000 (0.3764)  acc5: 1.0000 (0.8688)  time: 0.1076  data: 0.0680  max mem: 896
Test:  [1290/2793]  eta: 0:02:27  loss: 0.0258 (0.1075)  acc1: 0.5000 (0.3771)  acc5: 1.0000 (0.8696)  time: 0.0913  data: 0.0542  max mem: 896
Test:  [1300/2793]  eta: 0:02:26  loss: 0.0478 (0.1077)  acc1: 0.5000 (0.3767)  acc5: 1.0000 (0.8689)  time: 0.0865  data: 0.0473  max mem: 896
Test:  [1310/2793]  eta: 0:02:25  loss: 0.0923 (0.1077)  acc1: 0.2500 (0.3767)  acc5: 0.8000 (0.8684)  time: 0.0810  data: 0.0395  max mem: 896
Test:  [1320/2793]  eta: 0:02:24  loss: 0.0729 (0.1074)  acc1: 0.2500 (0.3761)  acc5: 1.0000 (0.8681)  time: 0.0826  data: 0.0414  max mem: 896
Test:  [1330/2793]  eta: 0:02:23  loss: 0.1127 (0.1083)  acc1: 0.2500 (0.3752)  acc5: 0.7500 (0.8657)  time: 0.0914  data: 0.0531  max mem: 896
Test:  [1340/2793]  eta: 0:02:22  loss: 0.1370 (0.1082)  acc1: 0.2500 (0.3757)  acc5: 0.7500 (0.8658)  time: 0.0934  data: 0.0537  max mem: 896
Test:  [1350/2793]  eta: 0:02:21  loss: 0.0768 (0.1085)  acc1: 0.3333 (0.3751)  acc5: 1.0000 (0.8661)  time: 0.0924  data: 0.0512  max mem: 896
Test:  [1360/2793]  eta: 0:02:20  loss: 0.0867 (0.1085)  acc1: 0.3333 (0.3755)  acc5: 1.0000 (0.8661)  time: 0.1044  data: 0.0649  max mem: 896
Test:  [1370/2793]  eta: 0:02:19  loss: 0.0272 (0.1078)  acc1: 0.5000 (0.3763)  acc5: 1.0000 (0.8671)  time: 0.0918  data: 0.0526  max mem: 896
Test:  [1380/2793]  eta: 0:02:17  loss: 0.0072 (0.1071)  acc1: 0.5000 (0.3772)  acc5: 1.0000 (0.8680)  time: 0.0738  data: 0.0358  max mem: 896
Test:  [1390/2793]  eta: 0:02:16  loss: 0.0123 (0.1071)  acc1: 0.5000 (0.3767)  acc5: 1.0000 (0.8686)  time: 0.0911  data: 0.0524  max mem: 896
Test:  [1400/2793]  eta: 0:02:15  loss: 0.0746 (0.1070)  acc1: 0.3333 (0.3769)  acc5: 1.0000 (0.8690)  time: 0.0971  data: 0.0533  max mem: 896
Test:  [1410/2793]  eta: 0:02:14  loss: 0.0793 (0.1072)  acc1: 0.2500 (0.3762)  acc5: 1.0000 (0.8676)  time: 0.0836  data: 0.0417  max mem: 896
Test:  [1420/2793]  eta: 0:02:13  loss: 0.1408 (0.1075)  acc1: 0.2500 (0.3756)  acc5: 0.7500 (0.8670)  time: 0.0876  data: 0.0495  max mem: 896
Test:  [1430/2793]  eta: 0:02:12  loss: 0.1130 (0.1073)  acc1: 0.3333 (0.3759)  acc5: 1.0000 (0.8676)  time: 0.0915  data: 0.0522  max mem: 896
Test:  [1440/2793]  eta: 0:02:11  loss: 0.1130 (0.1076)  acc1: 0.3333 (0.3757)  acc5: 1.0000 (0.8669)  time: 0.0974  data: 0.0604  max mem: 896
Test:  [1450/2793]  eta: 0:02:10  loss: 0.1540 (0.1079)  acc1: 0.3333 (0.3753)  acc5: 0.8000 (0.8665)  time: 0.1042  data: 0.0667  max mem: 896
Test:  [1460/2793]  eta: 0:02:09  loss: 0.1041 (0.1081)  acc1: 0.5000 (0.3760)  acc5: 0.8333 (0.8664)  time: 0.0952  data: 0.0553  max mem: 896
Test:  [1470/2793]  eta: 0:02:09  loss: 0.0925 (0.1084)  acc1: 0.5000 (0.3760)  acc5: 1.0000 (0.8664)  time: 0.1052  data: 0.0663  max mem: 896
Test:  [1480/2793]  eta: 0:02:07  loss: 0.1012 (0.1085)  acc1: 0.4000 (0.3755)  acc5: 1.0000 (0.8662)  time: 0.1007  data: 0.0619  max mem: 896
Test:  [1490/2793]  eta: 0:02:06  loss: 0.0867 (0.1082)  acc1: 0.5000 (0.3760)  acc5: 1.0000 (0.8667)  time: 0.0865  data: 0.0487  max mem: 896
Test:  [1500/2793]  eta: 0:02:05  loss: 0.0732 (0.1080)  acc1: 0.5000 (0.3762)  acc5: 1.0000 (0.8667)  time: 0.0836  data: 0.0469  max mem: 896
Test:  [1510/2793]  eta: 0:02:04  loss: 0.0389 (0.1076)  acc1: 0.5000 (0.3765)  acc5: 1.0000 (0.8669)  time: 0.0765  data: 0.0397  max mem: 896
Test:  [1520/2793]  eta: 0:02:03  loss: 0.0594 (0.1080)  acc1: 0.3333 (0.3760)  acc5: 1.0000 (0.8658)  time: 0.0924  data: 0.0541  max mem: 896
Test:  [1530/2793]  eta: 0:02:02  loss: 0.0589 (0.1076)  acc1: 0.5000 (0.3765)  acc5: 1.0000 (0.8664)  time: 0.1008  data: 0.0609  max mem: 896
Test:  [1540/2793]  eta: 0:02:01  loss: 0.0348 (0.1075)  acc1: 0.5000 (0.3771)  acc5: 1.0000 (0.8669)  time: 0.0877  data: 0.0480  max mem: 896
Test:  [1550/2793]  eta: 0:02:00  loss: 0.0539 (0.1074)  acc1: 0.5000 (0.3768)  acc5: 1.0000 (0.8666)  time: 0.0877  data: 0.0491  max mem: 896
Test:  [1560/2793]  eta: 0:01:59  loss: 0.0687 (0.1075)  acc1: 0.5000 (0.3774)  acc5: 1.0000 (0.8669)  time: 0.1018  data: 0.0638  max mem: 896
Test:  [1570/2793]  eta: 0:01:58  loss: 0.1035 (0.1076)  acc1: 0.5000 (0.3771)  acc5: 1.0000 (0.8663)  time: 0.0928  data: 0.0542  max mem: 896
Test:  [1580/2793]  eta: 0:01:57  loss: 0.1118 (0.1074)  acc1: 0.5000 (0.3777)  acc5: 1.0000 (0.8665)  time: 0.0953  data: 0.0576  max mem: 896
Test:  [1590/2793]  eta: 0:01:56  loss: 0.0990 (0.1077)  acc1: 0.5000 (0.3777)  acc5: 0.8333 (0.8659)  time: 0.1018  data: 0.0648  max mem: 896
Test:  [1600/2793]  eta: 0:01:55  loss: 0.0859 (0.1074)  acc1: 0.5000 (0.3778)  acc5: 0.7500 (0.8657)  time: 0.0819  data: 0.0431  max mem: 896
Test:  [1610/2793]  eta: 0:01:54  loss: 0.0696 (0.1073)  acc1: 0.5000 (0.3786)  acc5: 1.0000 (0.8665)  time: 0.0948  data: 0.0576  max mem: 896
Test:  [1620/2793]  eta: 0:01:53  loss: 0.0898 (0.1074)  acc1: 0.5000 (0.3783)  acc5: 1.0000 (0.8663)  time: 0.1035  data: 0.0684  max mem: 896
Test:  [1630/2793]  eta: 0:01:52  loss: 0.1379 (0.1079)  acc1: 0.2500 (0.3780)  acc5: 0.8333 (0.8656)  time: 0.0848  data: 0.0479  max mem: 896
Test:  [1640/2793]  eta: 0:01:51  loss: 0.1379 (0.1080)  acc1: 0.2500 (0.3780)  acc5: 1.0000 (0.8657)  time: 0.0938  data: 0.0544  max mem: 896
Test:  [1650/2793]  eta: 0:01:50  loss: 0.1292 (0.1083)  acc1: 0.5000 (0.3783)  acc5: 1.0000 (0.8655)  time: 0.1020  data: 0.0623  max mem: 896
Test:  [1660/2793]  eta: 0:01:49  loss: 0.1292 (0.1083)  acc1: 0.4000 (0.3784)  acc5: 1.0000 (0.8654)  time: 0.0904  data: 0.0537  max mem: 896
Test:  [1670/2793]  eta: 0:01:48  loss: 0.0461 (0.1079)  acc1: 0.5000 (0.3790)  acc5: 1.0000 (0.8660)  time: 0.0831  data: 0.0475  max mem: 896
Test:  [1680/2793]  eta: 0:01:47  loss: 0.0241 (0.1077)  acc1: 0.5000 (0.3794)  acc5: 1.0000 (0.8665)  time: 0.0821  data: 0.0443  max mem: 896
Test:  [1690/2793]  eta: 0:01:46  loss: 0.1071 (0.1082)  acc1: 0.5000 (0.3798)  acc5: 1.0000 (0.8660)  time: 0.0904  data: 0.0538  max mem: 896
Test:  [1700/2793]  eta: 0:01:45  loss: 0.1062 (0.1082)  acc1: 0.5000 (0.3801)  acc5: 1.0000 (0.8661)  time: 0.1046  data: 0.0709  max mem: 896
Test:  [1710/2793]  eta: 0:01:44  loss: 0.0833 (0.1082)  acc1: 0.5000 (0.3802)  acc5: 1.0000 (0.8665)  time: 0.0915  data: 0.0561  max mem: 896
Test:  [1720/2793]  eta: 0:01:43  loss: 0.0775 (0.1085)  acc1: 0.5000 (0.3804)  acc5: 1.0000 (0.8665)  time: 0.0878  data: 0.0522  max mem: 896
Test:  [1730/2793]  eta: 0:01:42  loss: 0.0644 (0.1082)  acc1: 0.5000 (0.3806)  acc5: 1.0000 (0.8673)  time: 0.0889  data: 0.0532  max mem: 896
Test:  [1740/2793]  eta: 0:01:41  loss: 0.0553 (0.1081)  acc1: 0.5000 (0.3807)  acc5: 1.0000 (0.8674)  time: 0.0895  data: 0.0542  max mem: 896
Test:  [1750/2793]  eta: 0:01:40  loss: 0.0705 (0.1080)  acc1: 0.5000 (0.3810)  acc5: 1.0000 (0.8676)  time: 0.1051  data: 0.0699  max mem: 896
Test:  [1760/2793]  eta: 0:01:39  loss: 0.1176 (0.1083)  acc1: 0.5000 (0.3814)  acc5: 0.7500 (0.8671)  time: 0.0927  data: 0.0565  max mem: 896
Test:  [1770/2793]  eta: 0:01:38  loss: 0.1425 (0.1085)  acc1: 0.5000 (0.3815)  acc5: 0.7500 (0.8666)  time: 0.0911  data: 0.0546  max mem: 896
Test:  [1780/2793]  eta: 0:01:37  loss: 0.1357 (0.1086)  acc1: 0.3333 (0.3815)  acc5: 0.7500 (0.8662)  time: 0.1017  data: 0.0657  max mem: 896
Test:  [1790/2793]  eta: 0:01:36  loss: 0.0634 (0.1084)  acc1: 0.3333 (0.3818)  acc5: 1.0000 (0.8669)  time: 0.0948  data: 0.0580  max mem: 896
Test:  [1800/2793]  eta: 0:01:35  loss: 0.0614 (0.1083)  acc1: 0.5000 (0.3821)  acc5: 1.0000 (0.8670)  time: 0.0891  data: 0.0506  max mem: 896
Test:  [1810/2793]  eta: 0:01:34  loss: 0.1001 (0.1083)  acc1: 0.5000 (0.3819)  acc5: 1.0000 (0.8668)  time: 0.0893  data: 0.0518  max mem: 896
Test:  [1820/2793]  eta: 0:01:33  loss: 0.0684 (0.1080)  acc1: 0.5000 (0.3822)  acc5: 1.0000 (0.8672)  time: 0.0857  data: 0.0495  max mem: 896
Test:  [1830/2793]  eta: 0:01:32  loss: 0.0719 (0.1083)  acc1: 0.5000 (0.3822)  acc5: 1.0000 (0.8669)  time: 0.0876  data: 0.0514  max mem: 896
Test:  [1840/2793]  eta: 0:01:31  loss: 0.0902 (0.1082)  acc1: 0.5000 (0.3822)  acc5: 1.0000 (0.8669)  time: 0.0866  data: 0.0500  max mem: 896
Test:  [1850/2793]  eta: 0:01:30  loss: 0.1025 (0.1084)  acc1: 0.3333 (0.3821)  acc5: 1.0000 (0.8665)  time: 0.0842  data: 0.0490  max mem: 896
Test:  [1860/2793]  eta: 0:01:29  loss: 0.1194 (0.1086)  acc1: 0.3333 (0.3819)  acc5: 0.7500 (0.8664)  time: 0.0969  data: 0.0611  max mem: 896
Test:  [1870/2793]  eta: 0:01:28  loss: 0.1103 (0.1088)  acc1: 0.3333 (0.3817)  acc5: 0.8333 (0.8661)  time: 0.0996  data: 0.0642  max mem: 896
Test:  [1880/2793]  eta: 0:01:27  loss: 0.1016 (0.1090)  acc1: 0.3333 (0.3814)  acc5: 0.8333 (0.8662)  time: 0.0847  data: 0.0475  max mem: 896
Test:  [1890/2793]  eta: 0:01:26  loss: 0.1118 (0.1093)  acc1: 0.2500 (0.3812)  acc5: 1.0000 (0.8661)  time: 0.0795  data: 0.0398  max mem: 896
Test:  [1900/2793]  eta: 0:01:25  loss: 0.1153 (0.1091)  acc1: 0.5000 (0.3814)  acc5: 1.0000 (0.8664)  time: 0.0840  data: 0.0449  max mem: 896
Test:  [1910/2793]  eta: 0:01:24  loss: 0.0541 (0.1092)  acc1: 0.5000 (0.3814)  acc5: 1.0000 (0.8665)  time: 0.0918  data: 0.0523  max mem: 896
Test:  [1920/2793]  eta: 0:01:23  loss: 0.0403 (0.1090)  acc1: 0.5000 (0.3816)  acc5: 1.0000 (0.8668)  time: 0.0988  data: 0.0607  max mem: 896
Test:  [1930/2793]  eta: 0:01:22  loss: 0.0399 (0.1088)  acc1: 0.5000 (0.3819)  acc5: 1.0000 (0.8674)  time: 0.0988  data: 0.0635  max mem: 896
Test:  [1940/2793]  eta: 0:01:21  loss: 0.0873 (0.1090)  acc1: 0.5000 (0.3820)  acc5: 1.0000 (0.8673)  time: 0.0994  data: 0.0652  max mem: 896
Test:  [1950/2793]  eta: 0:01:20  loss: 0.0782 (0.1088)  acc1: 0.5000 (0.3820)  acc5: 1.0000 (0.8674)  time: 0.0907  data: 0.0548  max mem: 896
Test:  [1960/2793]  eta: 0:01:19  loss: 0.0737 (0.1089)  acc1: 0.2500 (0.3816)  acc5: 1.0000 (0.8670)  time: 0.0790  data: 0.0419  max mem: 896
Test:  [1970/2793]  eta: 0:01:18  loss: 0.1138 (0.1089)  acc1: 0.2500 (0.3815)  acc5: 1.0000 (0.8673)  time: 0.0845  data: 0.0464  max mem: 896
Test:  [1980/2793]  eta: 0:01:17  loss: 0.1151 (0.1090)  acc1: 0.2500 (0.3809)  acc5: 0.8000 (0.8667)  time: 0.0955  data: 0.0567  max mem: 896
Test:  [1990/2793]  eta: 0:01:16  loss: 0.1393 (0.1095)  acc1: 0.2500 (0.3807)  acc5: 0.7500 (0.8661)  time: 0.0914  data: 0.0539  max mem: 896
Test:  [2000/2793]  eta: 0:01:16  loss: 0.1078 (0.1094)  acc1: 0.3333 (0.3806)  acc5: 1.0000 (0.8664)  time: 0.0912  data: 0.0539  max mem: 896
Test:  [2010/2793]  eta: 0:01:15  loss: 0.0488 (0.1093)  acc1: 0.5000 (0.3805)  acc5: 1.0000 (0.8665)  time: 0.0918  data: 0.0548  max mem: 896
Test:  [2020/2793]  eta: 0:01:14  loss: 0.0774 (0.1094)  acc1: 0.4000 (0.3805)  acc5: 1.0000 (0.8664)  time: 0.0980  data: 0.0631  max mem: 896
Test:  [2030/2793]  eta: 0:01:13  loss: 0.1032 (0.1096)  acc1: 0.3333 (0.3803)  acc5: 1.0000 (0.8662)  time: 0.1021  data: 0.0667  max mem: 896
Test:  [2040/2793]  eta: 0:01:12  loss: 0.1263 (0.1099)  acc1: 0.2500 (0.3794)  acc5: 0.6667 (0.8645)  time: 0.0924  data: 0.0538  max mem: 896
Test:  [2050/2793]  eta: 0:01:11  loss: 0.1107 (0.1099)  acc1: 0.2500 (0.3794)  acc5: 0.8000 (0.8650)  time: 0.0943  data: 0.0544  max mem: 896
Test:  [2060/2793]  eta: 0:01:10  loss: 0.0845 (0.1099)  acc1: 0.3333 (0.3793)  acc5: 1.0000 (0.8650)  time: 0.0940  data: 0.0551  max mem: 896
Test:  [2070/2793]  eta: 0:01:09  loss: 0.1675 (0.1103)  acc1: 0.2500 (0.3787)  acc5: 0.7500 (0.8643)  time: 0.0931  data: 0.0555  max mem: 896
Test:  [2080/2793]  eta: 0:01:08  loss: 0.1695 (0.1107)  acc1: 0.2500 (0.3785)  acc5: 0.7500 (0.8637)  time: 0.1133  data: 0.0760  max mem: 896
Test:  [2090/2793]  eta: 0:01:07  loss: 0.1275 (0.1108)  acc1: 0.3333 (0.3786)  acc5: 0.8333 (0.8638)  time: 0.1176  data: 0.0785  max mem: 896
Test:  [2100/2793]  eta: 0:01:06  loss: 0.1204 (0.1110)  acc1: 0.3333 (0.3783)  acc5: 1.0000 (0.8636)  time: 0.1008  data: 0.0602  max mem: 896
Test:  [2110/2793]  eta: 0:01:05  loss: 0.0990 (0.1110)  acc1: 0.3333 (0.3783)  acc5: 1.0000 (0.8634)  time: 0.1013  data: 0.0612  max mem: 896
Test:  [2120/2793]  eta: 0:01:04  loss: 0.1234 (0.1112)  acc1: 0.3333 (0.3782)  acc5: 0.8333 (0.8633)  time: 0.1045  data: 0.0649  max mem: 896
Test:  [2130/2793]  eta: 0:01:03  loss: 0.1117 (0.1110)  acc1: 0.4000 (0.3786)  acc5: 1.0000 (0.8636)  time: 0.0943  data: 0.0556  max mem: 896
Test:  [2140/2793]  eta: 0:01:02  loss: 0.0933 (0.1113)  acc1: 0.5000 (0.3785)  acc5: 1.0000 (0.8630)  time: 0.0910  data: 0.0527  max mem: 896
Test:  [2150/2793]  eta: 0:01:01  loss: 0.1218 (0.1114)  acc1: 0.2500 (0.3782)  acc5: 0.8000 (0.8626)  time: 0.1017  data: 0.0635  max mem: 896
Test:  [2160/2793]  eta: 0:01:00  loss: 0.1111 (0.1114)  acc1: 0.3333 (0.3784)  acc5: 1.0000 (0.8629)  time: 0.0950  data: 0.0564  max mem: 896
Test:  [2170/2793]  eta: 0:00:59  loss: 0.1203 (0.1116)  acc1: 0.3333 (0.3781)  acc5: 1.0000 (0.8625)  time: 0.0961  data: 0.0558  max mem: 896
Test:  [2180/2793]  eta: 0:00:58  loss: 0.1244 (0.1116)  acc1: 0.3333 (0.3781)  acc5: 0.8333 (0.8624)  time: 0.0973  data: 0.0569  max mem: 896
Test:  [2190/2793]  eta: 0:00:57  loss: 0.1224 (0.1117)  acc1: 0.3333 (0.3778)  acc5: 0.8333 (0.8625)  time: 0.0902  data: 0.0511  max mem: 896
Test:  [2200/2793]  eta: 0:00:56  loss: 0.1309 (0.1118)  acc1: 0.3333 (0.3777)  acc5: 1.0000 (0.8625)  time: 0.0933  data: 0.0535  max mem: 896
Test:  [2210/2793]  eta: 0:00:56  loss: 0.1015 (0.1118)  acc1: 0.3333 (0.3776)  acc5: 1.0000 (0.8626)  time: 0.1022  data: 0.0623  max mem: 896
Test:  [2220/2793]  eta: 0:00:55  loss: 0.0967 (0.1119)  acc1: 0.3333 (0.3776)  acc5: 1.0000 (0.8627)  time: 0.1000  data: 0.0607  max mem: 896
Test:  [2230/2793]  eta: 0:00:54  loss: 0.0818 (0.1117)  acc1: 0.5000 (0.3778)  acc5: 1.0000 (0.8630)  time: 0.1013  data: 0.0618  max mem: 896
Test:  [2240/2793]  eta: 0:00:53  loss: 0.0950 (0.1117)  acc1: 0.4000 (0.3778)  acc5: 1.0000 (0.8629)  time: 0.1053  data: 0.0655  max mem: 896
Test:  [2250/2793]  eta: 0:00:52  loss: 0.1234 (0.1118)  acc1: 0.3333 (0.3774)  acc5: 0.7500 (0.8626)  time: 0.0939  data: 0.0544  max mem: 896
Test:  [2260/2793]  eta: 0:00:51  loss: 0.1088 (0.1119)  acc1: 0.3333 (0.3775)  acc5: 1.0000 (0.8625)  time: 0.0990  data: 0.0593  max mem: 896
Test:  [2270/2793]  eta: 0:00:50  loss: 0.1105 (0.1120)  acc1: 0.3333 (0.3773)  acc5: 1.0000 (0.8626)  time: 0.0994  data: 0.0590  max mem: 896
Test:  [2280/2793]  eta: 0:00:49  loss: 0.1161 (0.1120)  acc1: 0.3333 (0.3774)  acc5: 1.0000 (0.8630)  time: 0.1017  data: 0.0620  max mem: 896
Test:  [2290/2793]  eta: 0:00:48  loss: 0.1027 (0.1120)  acc1: 0.4000 (0.3775)  acc5: 1.0000 (0.8631)  time: 0.1070  data: 0.0663  max mem: 896
Test:  [2300/2793]  eta: 0:00:47  loss: 0.1013 (0.1120)  acc1: 0.4000 (0.3775)  acc5: 0.8000 (0.8628)  time: 0.1027  data: 0.0620  max mem: 896
Test:  [2310/2793]  eta: 0:00:46  loss: 0.1096 (0.1121)  acc1: 0.3333 (0.3775)  acc5: 0.8333 (0.8630)  time: 0.1075  data: 0.0689  max mem: 896
Test:  [2320/2793]  eta: 0:00:45  loss: 0.0889 (0.1120)  acc1: 0.3333 (0.3776)  acc5: 1.0000 (0.8634)  time: 0.1103  data: 0.0720  max mem: 896
Test:  [2330/2793]  eta: 0:00:44  loss: 0.0848 (0.1119)  acc1: 0.4000 (0.3777)  acc5: 1.0000 (0.8635)  time: 0.0937  data: 0.0551  max mem: 896
Test:  [2340/2793]  eta: 0:00:43  loss: 0.0866 (0.1120)  acc1: 0.5000 (0.3778)  acc5: 1.0000 (0.8634)  time: 0.0934  data: 0.0548  max mem: 896
Test:  [2350/2793]  eta: 0:00:42  loss: 0.1125 (0.1123)  acc1: 0.5000 (0.3776)  acc5: 1.0000 (0.8632)  time: 0.1032  data: 0.0649  max mem: 896
Test:  [2360/2793]  eta: 0:00:41  loss: 0.0887 (0.1126)  acc1: 0.3333 (0.3775)  acc5: 1.0000 (0.8632)  time: 0.1120  data: 0.0736  max mem: 896
Test:  [2370/2793]  eta: 0:00:40  loss: 0.0858 (0.1126)  acc1: 0.5000 (0.3775)  acc5: 1.0000 (0.8637)  time: 0.1088  data: 0.0689  max mem: 896
Test:  [2380/2793]  eta: 0:00:39  loss: 0.1100 (0.1127)  acc1: 0.5000 (0.3778)  acc5: 1.0000 (0.8636)  time: 0.0869  data: 0.0466  max mem: 896
Test:  [2390/2793]  eta: 0:00:38  loss: 0.1146 (0.1129)  acc1: 0.5000 (0.3779)  acc5: 0.8333 (0.8634)  time: 0.0857  data: 0.0469  max mem: 896
Test:  [2400/2793]  eta: 0:00:37  loss: 0.1107 (0.1129)  acc1: 0.4000 (0.3779)  acc5: 1.0000 (0.8634)  time: 0.0992  data: 0.0598  max mem: 896
Test:  [2410/2793]  eta: 0:00:36  loss: 0.1228 (0.1132)  acc1: 0.3333 (0.3776)  acc5: 1.0000 (0.8634)  time: 0.1079  data: 0.0672  max mem: 896
Test:  [2420/2793]  eta: 0:00:36  loss: 0.1219 (0.1131)  acc1: 0.3333 (0.3777)  acc5: 1.0000 (0.8635)  time: 0.1019  data: 0.0619  max mem: 896
Test:  [2430/2793]  eta: 0:00:35  loss: 0.0864 (0.1132)  acc1: 0.5000 (0.3776)  acc5: 1.0000 (0.8633)  time: 0.0926  data: 0.0546  max mem: 896
Test:  [2440/2793]  eta: 0:00:34  loss: 0.1039 (0.1133)  acc1: 0.5000 (0.3777)  acc5: 1.0000 (0.8634)  time: 0.1047  data: 0.0656  max mem: 896
Test:  [2450/2793]  eta: 0:00:33  loss: 0.1193 (0.1134)  acc1: 0.5000 (0.3779)  acc5: 1.0000 (0.8632)  time: 0.1093  data: 0.0681  max mem: 896
Test:  [2460/2793]  eta: 0:00:32  loss: 0.0995 (0.1134)  acc1: 0.5000 (0.3782)  acc5: 1.0000 (0.8633)  time: 0.0977  data: 0.0557  max mem: 896
Test:  [2470/2793]  eta: 0:00:31  loss: 0.0957 (0.1134)  acc1: 0.5000 (0.3784)  acc5: 1.0000 (0.8634)  time: 0.0953  data: 0.0541  max mem: 896
Test:  [2480/2793]  eta: 0:00:30  loss: 0.1123 (0.1135)  acc1: 0.3333 (0.3782)  acc5: 1.0000 (0.8633)  time: 0.1048  data: 0.0631  max mem: 896
Test:  [2490/2793]  eta: 0:00:29  loss: 0.1400 (0.1136)  acc1: 0.3333 (0.3782)  acc5: 0.7500 (0.8631)  time: 0.0990  data: 0.0577  max mem: 896
Test:  [2500/2793]  eta: 0:00:28  loss: 0.1137 (0.1138)  acc1: 0.3333 (0.3779)  acc5: 0.7500 (0.8631)  time: 0.0899  data: 0.0479  max mem: 896
Test:  [2510/2793]  eta: 0:00:27  loss: 0.0971 (0.1138)  acc1: 0.3333 (0.3779)  acc5: 1.0000 (0.8631)  time: 0.0926  data: 0.0500  max mem: 896
Test:  [2520/2793]  eta: 0:00:26  loss: 0.0953 (0.1140)  acc1: 0.4000 (0.3779)  acc5: 1.0000 (0.8630)  time: 0.0892  data: 0.0498  max mem: 896
Test:  [2530/2793]  eta: 0:00:25  loss: 0.0828 (0.1141)  acc1: 0.3333 (0.3778)  acc5: 1.0000 (0.8627)  time: 0.0926  data: 0.0536  max mem: 896
Test:  [2540/2793]  eta: 0:00:24  loss: 0.1425 (0.1144)  acc1: 0.3333 (0.3777)  acc5: 0.7500 (0.8621)  time: 0.1102  data: 0.0708  max mem: 896
Test:  [2550/2793]  eta: 0:00:23  loss: 0.1314 (0.1143)  acc1: 0.4000 (0.3779)  acc5: 0.8000 (0.8624)  time: 0.1104  data: 0.0709  max mem: 896
Test:  [2560/2793]  eta: 0:00:22  loss: 0.0982 (0.1144)  acc1: 0.4000 (0.3778)  acc5: 1.0000 (0.8618)  time: 0.0935  data: 0.0533  max mem: 896
Test:  [2570/2793]  eta: 0:00:21  loss: 0.1454 (0.1146)  acc1: 0.2500 (0.3773)  acc5: 0.7500 (0.8614)  time: 0.0953  data: 0.0565  max mem: 896
Test:  [2580/2793]  eta: 0:00:20  loss: 0.1083 (0.1144)  acc1: 0.3333 (0.3775)  acc5: 1.0000 (0.8616)  time: 0.0996  data: 0.0586  max mem: 896
Test:  [2590/2793]  eta: 0:00:19  loss: 0.0917 (0.1145)  acc1: 0.5000 (0.3774)  acc5: 1.0000 (0.8613)  time: 0.0900  data: 0.0486  max mem: 896
Test:  [2600/2793]  eta: 0:00:18  loss: 0.1042 (0.1146)  acc1: 0.3333 (0.3773)  acc5: 1.0000 (0.8615)  time: 0.0958  data: 0.0578  max mem: 896
Test:  [2610/2793]  eta: 0:00:17  loss: 0.1042 (0.1145)  acc1: 0.3333 (0.3773)  acc5: 1.0000 (0.8615)  time: 0.1042  data: 0.0648  max mem: 896
Test:  [2620/2793]  eta: 0:00:16  loss: 0.0880 (0.1145)  acc1: 0.4000 (0.3772)  acc5: 1.0000 (0.8615)  time: 0.0992  data: 0.0592  max mem: 896
Test:  [2630/2793]  eta: 0:00:15  loss: 0.0830 (0.1145)  acc1: 0.5000 (0.3775)  acc5: 1.0000 (0.8615)  time: 0.1043  data: 0.0641  max mem: 896
Test:  [2640/2793]  eta: 0:00:14  loss: 0.0996 (0.1146)  acc1: 0.5000 (0.3774)  acc5: 0.7500 (0.8613)  time: 0.1062  data: 0.0667  max mem: 896
Test:  [2650/2793]  eta: 0:00:13  loss: 0.1417 (0.1148)  acc1: 0.2500 (0.3772)  acc5: 0.7500 (0.8610)  time: 0.1029  data: 0.0641  max mem: 896
Test:  [2660/2793]  eta: 0:00:12  loss: 0.1095 (0.1147)  acc1: 0.3333 (0.3771)  acc5: 0.8000 (0.8611)  time: 0.1038  data: 0.0638  max mem: 896
Test:  [2670/2793]  eta: 0:00:11  loss: 0.0969 (0.1149)  acc1: 0.3333 (0.3767)  acc5: 0.8000 (0.8602)  time: 0.1067  data: 0.0671  max mem: 896
Test:  [2680/2793]  eta: 0:00:10  loss: 0.1510 (0.1151)  acc1: 0.3333 (0.3767)  acc5: 0.8000 (0.8601)  time: 0.1015  data: 0.0614  max mem: 896
Test:  [2690/2793]  eta: 0:00:09  loss: 0.1469 (0.1152)  acc1: 0.3333 (0.3764)  acc5: 0.8333 (0.8598)  time: 0.0955  data: 0.0528  max mem: 896
Test:  [2700/2793]  eta: 0:00:09  loss: 0.1145 (0.1153)  acc1: 0.3333 (0.3765)  acc5: 0.8333 (0.8599)  time: 0.0963  data: 0.0564  max mem: 896
Test:  [2710/2793]  eta: 0:00:08  loss: 0.1321 (0.1155)  acc1: 0.5000 (0.3767)  acc5: 1.0000 (0.8597)  time: 0.1035  data: 0.0639  max mem: 896
Test:  [2720/2793]  eta: 0:00:07  loss: 0.1281 (0.1155)  acc1: 0.3333 (0.3766)  acc5: 0.7500 (0.8594)  time: 0.0928  data: 0.0525  max mem: 896
Test:  [2730/2793]  eta: 0:00:06  loss: 0.1235 (0.1155)  acc1: 0.5000 (0.3768)  acc5: 0.8333 (0.8592)  time: 0.0880  data: 0.0481  max mem: 896
Test:  [2740/2793]  eta: 0:00:05  loss: 0.0778 (0.1157)  acc1: 0.5000 (0.3768)  acc5: 1.0000 (0.8592)  time: 0.1002  data: 0.0581  max mem: 896
Test:  [2750/2793]  eta: 0:00:04  loss: 0.0757 (0.1156)  acc1: 0.5000 (0.3770)  acc5: 1.0000 (0.8594)  time: 0.0952  data: 0.0535  max mem: 896
Test:  [2760/2793]  eta: 0:00:03  loss: 0.0746 (0.1156)  acc1: 0.5000 (0.3772)  acc5: 1.0000 (0.8597)  time: 0.0914  data: 0.0510  max mem: 896
Test:  [2770/2793]  eta: 0:00:02  loss: 0.1026 (0.1158)  acc1: 0.5000 (0.3771)  acc5: 1.0000 (0.8596)  time: 0.0992  data: 0.0597  max mem: 896
Test:  [2780/2793]  eta: 0:00:01  loss: 0.0840 (0.1157)  acc1: 0.4000 (0.3771)  acc5: 1.0000 (0.8599)  time: 0.0981  data: 0.0587  max mem: 896
Test:  [2790/2793]  eta: 0:00:00  loss: 0.0870 (0.1158)  acc1: 0.3333 (0.3770)  acc5: 1.0000 (0.8600)  time: 0.0936  data: 0.0578  max mem: 896
Test:  [2792/2793]  eta: 0:00:00  loss: 0.0987 (0.1158)  acc1: 0.3333 (0.3770)  acc5: 1.0000 (0.8600)  time: 0.0835  data: 0.0492  max mem: 896
Test: Total time: 0:04:30 (0.0970 s / it)
* Acc@1 0.377 Acc@5 0.860 loss 0.116
Start merging results...
Reading individual output files
Computing final results
5586
