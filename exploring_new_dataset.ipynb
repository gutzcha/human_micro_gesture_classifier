{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DataAugmentationForVideoMAE\n",
    "from kinetics import VideoMAE2VID\n",
    "from types import SimpleNamespace as Namespace\n",
    "from torch.utils.data import DataLoader \n",
    "import torch\n",
    "from timm.models import create_model\n",
    "import os.path as osp\n",
    "import copy\n",
    "from einops import rearrange\n",
    "from utils import  clone_decoder_weights\n",
    "import run_videomae_vis_v2  as vis_utils \n",
    "# unnormalize_frames, save_video, reconstruct_video_from_patches\n",
    "import importlib\n",
    "from datasets import build_pretraining_dataset_multi_decoder\n",
    "# importlib.reload(VideoMAE2VID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(args):\n",
    "    print(f\"Creating model: {args.model}\")\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=False,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=None,\n",
    "        decoder_depth=args.decoder_depth,\n",
    "        clone_decoder = args.clone_decoder\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clone_decoder_weights(n, original_dict):\n",
    "\n",
    "#         # New dictionary to store updated keys and values\n",
    "#     new_dict = {}\n",
    "\n",
    "#         # Iterate over original dictionary\n",
    "#         # Iterate over original dictionary\n",
    "#     for key, value in original_dict.items():\n",
    "#         # Check if the key contains the word 'decoder.'\n",
    "#         if key.startswith('decoder.'):\n",
    "            \n",
    "#             # Generate new keys based on the pattern\n",
    "#             new_keys = [key.replace('decoder.', f'decoder.decoders.{i}.') for i in range(n)]\n",
    "            \n",
    "#             # Update new dictionary with new keys and cloned values\n",
    "#             for new_key in new_keys:\n",
    "#                 new_dict[new_key] = value.clone()\n",
    "#         else:\n",
    "#             # If key doesn't contain 'decoder.', simply copy the original key-value pair\n",
    "#             new_dict[key] = value\n",
    "#     return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_pretraining_dataset(args):\n",
    "#     transform = DataAugmentationForVideoMAE(args)\n",
    "#     dataset = VideoMAE2VID(\n",
    "#         root=args.root,\n",
    "#         setting=args.data_path,\n",
    "#         is_color=True,\n",
    "#         modality='rgb',\n",
    "#         new_length=args.num_frames,\n",
    "#         new_step=args.sampling_rate,\n",
    "#         transform=transform,\n",
    "#         temporal_jitter=False,\n",
    "#         video_loader=True,\n",
    "#         use_decord=True,\n",
    "#         lazy_init=False,\n",
    "#         features_cfg=args.features_cfg,\n",
    "\n",
    "#         )\n",
    "#     print(\"Data Aug = %s\" % str(transform))\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_name = 'VideoMAE_ViT-B_checkpoint_Kinetics-400.pth'\n",
    "# model_path = osp.join('/videos/pretrained/pretrained',checkpoint_name)\n",
    "model_path = osp.join('/videos/pretrained/MPIIGroupInteraction/k400_finetune_videomae_pretrain_dual_patch16_224_frame_16x4_tube_mask_ratio_0.9_e100/checkpoint-99.pth')\n",
    "\n",
    "model_name = 'pretrain_videomae_base_patch16_224_densepose_dual'\n",
    "# model_name = 'pretrain_videomae_base_patch16_224'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cfg ={\n",
    "    'videos': {'folder': 'clips_train', 'file_extention':'mp4', 'loss_weight':0.5},\n",
    "    'densepose': {'folder': 'densepose_train', 'file_extention':'mp4', 'loss_weight':0.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m args\u001b[39m.\u001b[39mpatch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m args\u001b[39m.\u001b[39mwindow_size \u001b[39m=\u001b[39m (args\u001b[39m.\u001b[39mnum_frames \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, args\u001b[39m.\u001b[39minput_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m args\u001b[39m.\u001b[39mpatch_size, args\u001b[39m.\u001b[39minput_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m args\u001b[39m.\u001b[39mpatch_size)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m args\u001b[39m.\u001b[39mmodel_path\u001b[39m=\u001b[39mmodel_path\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m args\u001b[39m.\u001b[39mdevice\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m args\u001b[39m.\u001b[39mimagenet_default_mean_and_std\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "args = Namespace()\n",
    "args.root = '/videos/mpi_data/2Itzik/MPIIGroupInteraction/'\n",
    "args.data_path = 'train_data_mpig.txt'\n",
    "args.features_cfg = features_cfg\n",
    "args.mask_type = 'tube' \n",
    "args.mask_ratio = 0.9 \n",
    "args.decoder_depth = 4 \n",
    "args.batch_size = 32 \n",
    "args.num_frames = 16 \n",
    "args.sampling_rate = 4 \n",
    "args.input_size = 224\n",
    "args.patch_size = 16\n",
    "args.window_size = (args.num_frames // 2, args.input_size // args.patch_size, args.input_size // args.patch_size)\n",
    "args.model_path=model_path\n",
    "args.device='cuda:0'\n",
    "args.imagenet_default_mean_and_std=True\n",
    "args.model=model_name\n",
    "args.drop_path=0.0\n",
    "args.clone_decoder = False\n",
    "args.feature_cfg = features_cfg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'types.SimpleNamespace' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m get_model(args)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m patch_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mpatch_embed\u001b[39m.\u001b[39mpatch_size\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'types.SimpleNamespace' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "model = get_model(args)\n",
    "patch_size = model.encoder.patch_embed.patch_size\n",
    "print(\"Patch size = %s\" % str(patch_size))\n",
    "args.window_size = (args.num_frames // 2, args.input_size // patch_size[0], args.input_size // patch_size[1])\n",
    "args.patch_size = patch_size\n",
    "\n",
    "model.to(device)\n",
    "checkpoint = torch.load(args.model_path, map_location='cpu')\n",
    "n = model.decoder.num_decoders\n",
    "state_dict = clone_decoder_weights(n, checkpoint['model'])\n",
    "# state_dict = checkpoint['model']\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mstate_dict()[\u001b[39m'\u001b[39m\u001b[39mdecoder.decoders.1.blocks.\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.state_dict()['decoder.decoders.1.blocks.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bitzikg_cs_p4/home/ubuntu/efs/videoMAE/exploring_new_dataset.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mnum_decoders\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.decoder.num_decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = build_pretraining_dataset(args=args)\n",
    "dataset = build_pretraining_dataset_multi_decoder(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(dataloader.batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator_dataset =iter(dataset)\n",
    "# batch =  next(iterator_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i , (batch, mask) in enumerate(dataloader):\n",
    "    print('new batch')\n",
    "    for b in batch:\n",
    "        print(b.shape)\n",
    "    print(i)\n",
    "    if i >= 1:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ori_img = vis_utils.unnormalize_frames(img=batch[0])\n",
    "# ori_img = vis_utils.unnormalize_frames(img=batch[1])\n",
    "video_save_path = 'testing_this_deletelater.mp4'\n",
    "vis_utils.save_video(ori_img[0], video_save_path,frame_id_list=None, prepend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = batch[0].to(device)\n",
    "mask_input = mask.flatten(1).to(torch.bool).to(device)\n",
    "mask_input = torch.zeros_like(mask_input).to(torch.bool).to(device)\n",
    "outputs = model(model_input, mask_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_squeeze = rearrange(batch[0], 'b c (t p0) (h p1) (w p2) -> b (t h w) (p0 p1 p2) c', p0=2, p1=patch_size[0], p2=patch_size[0])\n",
    "# img_patch = rearrange(img_squeeze, 'b n p c -> b n (p c)')\n",
    "\n",
    "# model_output = outputs[0]\n",
    "\n",
    "# model_output = model_output.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(outputs[0] == outputs[1]))\n",
    "print(outputs[0].numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(vis_utils)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bool_masked_pos = mask_input\n",
    "bool_masked_pos = torch.zeros_like(mask_input)\n",
    "frame_id_list = None\n",
    "model_out = outputs[1]\n",
    "imgs, rec_img, mask_2 = vis_utils.reconstruct_video_from_patches(ori_img.to(device), patch_size, bool_masked_pos, model_out, frame_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec_img.shape\n",
    "vis_utils.save_video(rec_img, video_save_path,frame_id_list=None, prepend=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry test with run_pretrain\n",
    "checkpoint_name = 'VideoMAE_ViT-B_checkpoint_Kinetics-400.pth'\n",
    "model_path = osp.join('/videos/pretrained/pretrained',checkpoint_name)\n",
    "\n",
    "model_name = 'pretrain_videomae_base_patch16_224_densepose_dual'\n",
    "\n",
    "import argparse\n",
    "\n",
    "# Create args Namespace with default values\n",
    "args = argparse.Namespace(\n",
    "    batch_size=64,\n",
    "    epochs=800,\n",
    "    save_ckpt_freq=50,\n",
    "    decoder_depth=4,\n",
    "    mask_type='tube',\n",
    "    mask_ratio=0.75,\n",
    "    input_size=224,\n",
    "    drop_path=0.0,\n",
    "    normlize_target=True,\n",
    "    opt='adamw',\n",
    "    opt_eps=1e-8,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.05,\n",
    "    lr=1.5e-4,\n",
    "    warmup_lr=1e-6,\n",
    "    min_lr=1e-5,\n",
    "    warmup_epochs=40,\n",
    "    warmup_steps=-1,\n",
    "    use_checkpoint=False,\n",
    "    color_jitter=0.0,\n",
    "    train_interpolation='bicubic',\n",
    "    data_path='/path/to/list_kinetics-400',\n",
    "    imagenet_default_mean_and_std=True,\n",
    "    num_frames=16,\n",
    "    sampling_rate=4,\n",
    "    output_dir='',\n",
    "    log_dir=None,\n",
    "    device='cuda',\n",
    "    seed=0,\n",
    "    auto_resume=True,\n",
    "    start_epoch=0,\n",
    "    num_workers=10,\n",
    "    pin_mem=True,\n",
    "    world_size=1,\n",
    "    local_rank=-1,\n",
    "    dist_on_itp=False,\n",
    "    dist_url='env://',\n",
    "    features_cfg=None,\n",
    "    clone_decoder=False,\n",
    "    weight_decay_end=None,\n",
    "    resume='',\n",
    "    clip_grad=None,\n",
    ")\n",
    "\n",
    "# Override default values with values provided in the call from terminal\n",
    "args.batch_size = 2\n",
    "args.lr = 0.000003\n",
    "args.save_ckpt_freq = 20\n",
    "args.epochs = 500\n",
    "args.log_dir = 'testing_new_model_debug'\n",
    "args.output_dir = 'testing_new_model_debug'\n",
    "\n",
    "args.root = '/videos/mpi_data/2Itzik/MPIIGroupInteraction/'\n",
    "args.data_path = 'train_data_mpig.txt'\n",
    "args.mask_type = 'tube' \n",
    "args.mask_ratio = 0.9 \n",
    "args.decoder_depth = 4 \n",
    "args.batch_size = 2 \n",
    "args.num_frames = 16 \n",
    "args.sampling_rate = 4 \n",
    "args.input_size = 224\n",
    "args.patch_size = 16\n",
    "args.window_size = (args.num_frames // 2, args.input_size // args.patch_size, args.input_size // args.patch_size)\n",
    "args.model_path=model_path\n",
    "args.device='cuda:0'\n",
    "args.imagenet_default_mean_and_std=True\n",
    "args.model=model_name\n",
    "args.drop_path=0.0\n",
    "args.clone_decoder = False # True\n",
    "args.feature_cfg = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.feature_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_mae_pretraining_multi_decoder\n",
    "importlib.reload(run_mae_pretraining_multi_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    }
   ],
   "source": [
    "run_mae_pretraining_multi_decoder.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
