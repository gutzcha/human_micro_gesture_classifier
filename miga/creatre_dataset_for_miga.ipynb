{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:51:05.334154100Z",
     "start_time": "2024-05-01T11:51:05.325148500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import sys\n",
    "from const import ID2LABELS_SMG as LABEL2ID\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, '..')\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:19:53.178697900Z",
     "start_time": "2024-05-01T11:19:53.165687100Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_root_folder = '\\Project-mpg microgesture\\smg\\smg_split_files'\n",
    "path_to_root_folder = osp.join(*path_to_root_folder.split('/'))\n",
    "\n",
    "train_data_path = osp.join(path_to_root_folder,'train.csv')\n",
    "val_data_path = osp.join(path_to_root_folder,'validation.csv')\n",
    "\n",
    "train_files_folder = osp.join(path_to_root_folder, 'train')\n",
    "val_files_folder = osp.join(path_to_root_folder, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:19:53.214686400Z",
     "start_time": "2024-05-01T11:19:53.182688300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(train_data_path, index_col=0)\n",
    "df_val_raw = pd.read_csv(val_data_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:19:53.217686900Z",
     "start_time": "2024-05-01T11:19:53.199687800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   label  start_frame  end_frame  \\\n0     15          100        176   \n1     12          837        889   \n2     12         1710       1749   \n3      8         4849       4885   \n4      9         6330       6368   \n\n                                               paths    basename  durations  \n0  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         76  \n1  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         52  \n2  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         39  \n3  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         36  \n4  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         38  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>start_frame</th>\n      <th>end_frame</th>\n      <th>paths</th>\n      <th>basename</th>\n      <th>durations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>100</td>\n      <td>176</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>837</td>\n      <td>889</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12</td>\n      <td>1710</td>\n      <td>1749</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>4849</td>\n      <td>4885</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>6330</td>\n      <td>6368</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>38</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['label', 'start_frame', 'end_frame', 'paths', 'basename', 'durations'], dtype='object')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T11:19:53.253690900Z",
     "start_time": "2024-05-01T11:19:53.219689Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:19:54.798640100Z",
     "start_time": "2024-05-01T11:19:54.789599900Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Structure of the final dataset: filename, labels\n",
    "# def get_video_list(files_folder):\n",
    "#     all_files = glob(osp.join(files_folder, '*.mp4'))\n",
    "#     base_names = [osp.basename(f) for f in all_files]\n",
    "#     sample_id = [int(osp.splitext(f)[0].split('-')[0]) for f in base_names]\n",
    "#     view = [(osp.splitext(f)[0].split('-video')[-1]) for f in base_names]\n",
    "#     df = pd.DataFrame(zip(all_files, base_names, sample_id, view), columns=['filenames', 'base_name', 'sample_id', 'view'])\n",
    "#     return df\n",
    "# df_train_files = get_video_list(files_folder=train_files_folder)\n",
    "# df_val_files = get_video_list(files_folder=val_files_folder)\n",
    "# # file_sample_ids.sort()\n",
    "# # file_sample_ids\n",
    "# # df_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "def convert_df(df_in):\n",
    "        \n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    df_out['filenames'] = df_in['paths'].apply(lambda x: osp.basename(x))\n",
    "    df_out['folder_name'] = df_in['paths'].apply(lambda x: x.split('\\\\')[-2])\n",
    "    df_out['durations'] = df_in['durations']\n",
    "    \n",
    "    df_out['view'] = 'center'\n",
    "    metadata = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    metadata['sample_id'] = df_out['filenames'].apply(lambda x: x.split('.')[0].replace('Sample','').replace('color_',''))\n",
    "    metadata['start_frame'] = df_in['start_frame']\n",
    "    metadata['end_frame'] = df_in['end_frame']\n",
    "    metadata['label_txt'] = df_in['label'].apply(lambda x: LABEL2ID[x])\n",
    "    metadata['view'] =  df_out['view']\n",
    "    metadata['filenames'] = df_out['filenames']\n",
    "    metadata['durations'] = df_out['durations']\n",
    "    \n",
    "    \n",
    "    one_hot_encoded  = pd.get_dummies(df_in['label'], columns=LABEL2ID.keys())\n",
    "    new_columns = one_hot_encoded.columns.to_list()\n",
    "    one_hot_encoded = one_hot_encoded.astype(float)\n",
    "    df_out['labels'] = one_hot_encoded.values.tolist()\n",
    "    df_out['metadata'] = metadata.to_dict(orient='records')\n",
    "    \n",
    "    return df_out, new_columns, one_hot_encoded\n",
    "\n",
    "df_train,new_columns, train_one_hot_encoded = convert_df(df_train_raw)\n",
    "df_val, _, val_one_hot_encoded = convert_df(df_val_raw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T12:32:28.699883800Z",
     "start_time": "2024-05-01T12:32:28.656883600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:41:02.199199100Z",
     "start_time": "2024-05-01T11:41:02.174200100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                   filenames folder_name  durations    view  \\\n0  Sample0001_color_0001.mp4       train         76  center   \n1  Sample0001_color_0002.mp4       train         52  center   \n2  Sample0001_color_0003.mp4       train         39  center   \n3  Sample0001_color_0004.mp4       train         36  center   \n4  Sample0001_color_0005.mp4       train         38  center   \n\n                                              labels  \\\n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n\n                                            metadata  \n0  {'sample_id': '0001_0001', 'start_frame': 100,...  \n1  {'sample_id': '0001_0002', 'start_frame': 837,...  \n2  {'sample_id': '0001_0003', 'start_frame': 1710...  \n3  {'sample_id': '0001_0004', 'start_frame': 4849...  \n4  {'sample_id': '0001_0005', 'start_frame': 6330...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filenames</th>\n      <th>folder_name</th>\n      <th>durations</th>\n      <th>view</th>\n      <th>labels</th>\n      <th>metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sample0001_color_0001.mp4</td>\n      <td>train</td>\n      <td>76</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0001', 'start_frame': 100,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sample0001_color_0002.mp4</td>\n      <td>train</td>\n      <td>52</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0002', 'start_frame': 837,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sample0001_color_0003.mp4</td>\n      <td>train</td>\n      <td>39</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0003', 'start_frame': 1710...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sample0001_color_0004.mp4</td>\n      <td>train</td>\n      <td>36</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0004', 'start_frame': 4849...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sample0001_color_0005.mp4</td>\n      <td>train</td>\n      <td>38</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n      <td>{'sample_id': '0001_0005', 'start_frame': 6330...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "    1    2    3    4    5    6    7    8    9    10   11   12   13   14   15  \\\n0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   \n1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0   \n3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n    16   17  \n0  0.0  0.0  \n1  0.0  0.0  \n2  0.0  0.0  \n3  0.0  0.0  \n4  0.0  0.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_one_hot_encoded.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T12:32:37.914626800Z",
     "start_time": "2024-05-01T12:32:37.886665400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['filenames', 'folder_name', 'durations', 'view', 'labels', 'metadata'], dtype='object')"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T11:41:02.384198300Z",
     "start_time": "2024-05-01T11:41:02.357201100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:41:02.505202Z",
     "start_time": "2024-05-01T11:41:02.463203200Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_folder = '\\Project-mpg microgesture\\human_micro_gesture_classifier\\scripts\\miga_smg'\n",
    "experiment_folder = osp.join(*experiment_folder.split('\\\\'))\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "df_val.to_csv(osp.join(experiment_folder,'val.csv'))\n",
    "df_train.to_csv(osp.join(experiment_folder,'train.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:41:02.604199100Z",
     "start_time": "2024-05-01T11:41:02.585199400Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import dyadic_communication\n",
    "from importlib import reload\n",
    "import debug_model\n",
    "import run_videomae_vis_v2\n",
    "\n",
    "reload(dyadic_communication)\n",
    "reload(debug_model)\n",
    "reload(run_videomae_vis_v2)\n",
    "\n",
    "from argparse import Namespace\n",
    "import mpigroup.const as const\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:41:02.689202Z",
     "start_time": "2024-05-01T11:41:02.681200100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'front1': 'tr', 'front2': 'tl', 'right': 'bl', 'left': 'br', 'center': 'mm'}"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(const)\n",
    "cropping_map = const.cropping_map\n",
    "cropping_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T11:41:02.842199300Z",
     "start_time": "2024-05-01T11:41:02.795199800Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path= r'D:\\Project-mpg microgesture\\smg\\smg_split_files'\n",
    "\n",
    "args = debug_model.args\n",
    "args.anno_path = osp.join(experiment_folder,'val.csv')\n",
    "args.data_path = data_path\n",
    "args.data_root = data_path\n",
    "args.mode = 'validation'\n",
    "args.test_mode = False\n",
    "dataset = dyadic_communication.DyadicvideoClsDataset(\n",
    "          anno_path=args.anno_path,\n",
    "            data_path=args.data_path,\n",
    "            mode=args.mode,\n",
    "            clip_len=1,\n",
    "            num_segment=args.num_frames,\n",
    "            test_num_segment=args.test_num_segment,\n",
    "            test_num_crop=args.test_num_crop,\n",
    "            num_crop=1 if not args.test_mode else 3,\n",
    "            keep_aspect_ratio=True,\n",
    "            crop_size=args.input_size,\n",
    "            short_side_size=args.short_side_size,\n",
    "            new_height=224,\n",
    "            new_width=224,\n",
    "            view_crop_mapping=cropping_map,\n",
    "            corner_crop_size=None,\n",
    "            data_root=args.data_root,\n",
    "            args=args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T11:41:02.936198200Z",
     "start_time": "2024-05-01T11:41:02.929199900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:27:40.327975Z",
     "start_time": "2024-05-01T12:27:40.317439500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = DataLoader(dataset=dataset, batch_size=5, shuffle=True)\n",
    "iterdata = iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:27:44.335006900Z",
     "start_time": "2024-05-01T12:27:41.492006100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 16, 224, 224])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       dtype=torch.float64)\n",
      "[('D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0158', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0326', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0308', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0257', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0306'), {'sample_id': ['0033_0158', '0033_0326', '0033_0308', '0033_0257', '0033_0306'], 'start_frame': tensor([12168, 24369, 22934, 20123, 22859]), 'end_frame': tensor([12222, 24396, 22965, 20166, 22891]), 'label_txt': ['Arms akimbo', 'Moving legs', 'Moving legs', 'Arms akimbo', 'Moving legs'], 'view': ['center', 'center', 'center', 'center', 'center'], 'filenames': ['Sample0033_color_0158.mp4', 'Sample0033_color_0326.mp4', 'Sample0033_color_0308.mp4', 'Sample0033_color_0257.mp4', 'Sample0033_color_0306.mp4'], 'durations': tensor([54, 27, 31, 43, 32])}, [], []]\n"
     ]
    }
   ],
   "source": [
    "d = next(iterdata)\n",
    "print(d[0].shape)\n",
    "print(d[1])\n",
    "print(d[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:27:46.326006400Z",
     "start_time": "2024-05-01T12:27:45.278013500Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(run_videomae_vis_v2)\n",
    "\n",
    "vids = d[0]\n",
    "labels = d[1]\n",
    "label_txt = d[3]['label_txt']\n",
    "fnames = d[3]['filenames']\n",
    "\n",
    "for vid,label, txt_label, fname in zip(vids,labels, label_txt, fnames):\n",
    "       \n",
    "    # ret = get_activities(feature_names, label)\n",
    "\n",
    "    txt =  '\\n'.join([fname ,txt_label])\n",
    "    \n",
    "    vid = run_videomae_vis_v2.unnormalize_frames(vid)\n",
    "    run_videomae_vis_v2.save_video(vid, osp.join('testing_'+fname), txt=txt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:27:46.341004Z",
     "start_time": "2024-05-01T12:27:46.329004500Z"
    }
   },
   "outputs": [],
   "source": [
    "# video_path = osp.join(path_to_root_folder,'clips_val','05942-video1.mp4')\n",
    "# video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:27:47.565008100Z",
     "start_time": "2024-05-01T12:27:47.553005500Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'experiment':'MPIG_densepose_dual_2',\n",
    "    'description':'MPIG_densepose_dual - videoMAE-K400 , same as K400 but then was finetuned on MPIGroupInteractions dataset (train set) for 100 epochs, with denspose as additional decoding target',\n",
    "    'checkpoint_path':r'D:\\Project-mpg microgesture\\pretrained\\pretrained\\MPIIGroupInteraction\\k400_finetune_videomae_pretrain_dual_2_patch16_224_frame_16x4_tube_mask_ratio_0.9_e100\\checkpoint-99.pth',\n",
    "    'model_name':'pretrain_videomae_base_patch16_224_densepose_dual',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:27:47.588003700Z",
     "start_time": "2024-05-01T12:27:47.569004700Z"
    }
   },
   "outputs": [],
   "source": [
    "image_batch = d[0]\n",
    "save_folder = osp.join('videos')\n",
    "model_path = model_dict['checkpoint_path']\n",
    "model_name = model_dict['model_name']\n",
    "\n",
    "args = Namespace(\n",
    "        image_batch=image_batch,\n",
    "        save_path=save_folder, # list\n",
    "        model_path=model_path, \n",
    "        mask_type='tube',\n",
    "        num_frames=16,\n",
    "        sampling_rate=4,\n",
    "        decoder_depth=4,\n",
    "        input_size=224,\n",
    "        device='cuda:0',\n",
    "        imagenet_default_mean_and_std=True,\n",
    "        mask_ratio=0,\n",
    "        model=model_name,\n",
    "        densepose=True,\n",
    "        drop_path=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:27:55.410599100Z",
     "start_time": "2024-05-01T12:27:49.028609200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: pretrain_videomae_base_patch16_224_densepose_dual\n"
     ]
    },
    {
     "data": {
      "text/plain": "PretrainVisionTransformerMultiOutout(\n  (encoder): PretrainVisionTransformerEncoder(\n    (patch_embed): PatchEmbed(\n      (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (head): Identity()\n  )\n  (decoder): PretrainVisionTransformerDecoderMultiOutput(\n    (decoders): ModuleList(\n      (0-1): 2 x PretrainVisionTransformerDecoder(\n        (blocks): ModuleList(\n          (0-3): 4 x Block(\n            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=384, out_features=384, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        (head): Linear(in_features=384, out_features=1536, bias=True)\n      )\n    )\n  )\n  (encoder_to_decoder): Linear(in_features=768, out_features=384, bias=False)\n)"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = run_videomae_vis_v2.get_model(args=args) \n",
    "\n",
    "checkpoint = torch.load(args.model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "\n",
    "# outputs = model(image_batch)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:28:10.557806900Z",
     "start_time": "2024-05-01T12:27:55.413173500Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(image_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:28:10.914804200Z",
     "start_time": "2024-05-01T12:28:10.563806100Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(run_videomae_vis_v2)\n",
    "\n",
    "\n",
    "\n",
    "rec_videos_patches = outputs[0]\n",
    "rec_densepose_patches = outputs[1]\n",
    "patch_size = model.encoder.patch_embed.patch_size\n",
    "unnorm_videos = run_videomae_vis_v2.unnormalize_frames(img=image_batch)\n",
    "_, rec_videos, _ = run_videomae_vis_v2.reconstruct_video_from_patches(\n",
    "    ori_img=unnorm_videos, patch_size=patch_size, bool_masked_pos=None, outputs=rec_videos_patches, frame_id_list=None)\n",
    "\n",
    "_, rec_densepose, _ = run_videomae_vis_v2.reconstruct_video_from_patches(\n",
    "    ori_img=unnorm_videos, patch_size=patch_size, bool_masked_pos=None, \n",
    "    outputs=rec_densepose_patches, frame_id_list=None,normalize_with_orig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 16, 224, 224])"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_densepose[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T12:28:10.929805300Z",
     "start_time": "2024-05-01T12:28:10.917805400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "# a = (torch.nn.functional.normalize(rec_densepose[0],dim=0) + torch.Tensor([1,1,1])[:,None,None,None]) / torch.Tensor([2,2,2])[:,None,None,None]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T12:28:10.965804400Z",
     "start_time": "2024-05-01T12:28:10.933805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "# print(f'mean: {a.mean(dim=[1,2,3])}')\n",
    "# print(f'std: {a.std(dim=[1,2,3])}')\n",
    "# print(f'min: {a.reshape(3, -1).min(dim=1)[0]}')\n",
    "# print(f'max: {a.reshape(3, -1).max(dim=1)[0] }')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T12:28:10.968803600Z",
     "start_time": "2024-05-01T12:28:10.946806400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:28:11.010807300Z",
     "start_time": "2024-05-01T12:28:10.964804600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([-3.0166e-05, -7.5249e-05,  4.2165e-05], grad_fn=<MeanBackward1>)\n",
      "std: tensor([0.1504, 0.1639, 0.1577], grad_fn=<StdBackward0>)\n",
      "min: tensor([-2.4738, -2.9901, -1.9143], grad_fn=<MinBackward0>)\n",
      "max: tensor([5.5295, 6.0539, 5.8453], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'mean: {rec_densepose[0].mean(dim=[1,2,3])}')\n",
    "print(f'std: {rec_densepose[0].std(dim=[1,2,3])}')\n",
    "print(f'min: {rec_densepose[0].reshape(3, -1).min(dim=1)[0]}')\n",
    "print(f'max: {rec_densepose[0].reshape(3, -1).max(dim=1)[0] }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:29:58.634263900Z",
     "start_time": "2024-05-01T12:29:56.078214900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "vids = d[0]\n",
    "labels = d[1]\n",
    "label_txt = d[3]['label_txt']\n",
    "fnames = d[3]['filenames']\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "for ori_vid,rec_vid,rec_dense,labels,txt_label, fname in zip(vids,rec_videos,rec_densepose,labels, label_txt, fnames):\n",
    "       \n",
    "    txt =  '\\n'.join([fname ,txt_label])\n",
    "\n",
    "    ori_vid = run_videomae_vis_v2.unnormalize_frames(ori_vid)\n",
    "    rec_dense = run_videomae_vis_v2.unnormalize_frames(rec_dense)\n",
    "    # rec_dense = (torch.nn.functional.normalize(rec_dense,dim=0) + torch.Tensor([1,1,1])[:,None,None,None]) / torch.Tensor([2,2,2])[:,None,None,None]\n",
    "    # rec_vid = run_videomae_vis_v2.unnormalize_frames(rec_vid)\n",
    "    # rec_dense = softmax(rec_dense)\n",
    "\n",
    "    save_folder = osp.join('videos',fname.replace('.mp4',''))\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    run_videomae_vis_v2.save_video(ori_vid, osp.join(save_folder,'ori_vid_'+fname), txt=txt)\n",
    "\n",
    "    # run_videomae_vis_v2.save_video(ori_dense, osp.join(save_folder,'ori_dense_'+fname), txt=txt)\n",
    "\n",
    "    run_videomae_vis_v2.save_video(rec_vid, osp.join(save_folder,'rec_vid_'+fname), txt=txt)\n",
    "\n",
    "    run_videomae_vis_v2.save_video(rec_dense, osp.join(save_folder,'rec_dense_'+fname), txt=txt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:30:44.099988600Z",
     "start_time": "2024-05-01T12:30:44.033048800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def generate_class_weights(class_series, multi_class=True, one_hot_encoded=False):\n",
    "  \"\"\"\n",
    "  Method to generate class weights given a set of multi-class or multi-label labels, both one-hot-encoded or not.\n",
    "  Some examples of different formats of class_series and their outputs are:\n",
    "    - generate_class_weights(['mango', 'lemon', 'banana', 'mango'], multi_class=True, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 1.3333333333333333, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]], multi_class=True, one_hot_encoded=True)\n",
    "    {0: 0.6666666666666666, 1: 1.3333333333333333, 2: 1.3333333333333333}\n",
    "    - generate_class_weights([['mango', 'lemon'], ['mango'], ['lemon', 'banana'], ['lemon']], multi_class=False, one_hot_encoded=False)\n",
    "    {'banana': 1.3333333333333333, 'lemon': 0.4444444444444444, 'mango': 0.6666666666666666}\n",
    "    - generate_class_weights([[0, 1, 1], [0, 0, 1], [1, 1, 0], [0, 1, 0]], multi_class=False, one_hot_encoded=True)\n",
    "    {0: 1.3333333333333333, 1: 0.4444444444444444, 2: 0.6666666666666666}\n",
    "  The output is a dictionary in the format { class_label: class_weight }. In case the input is one hot encoded, the class_label would be index\n",
    "  of appareance of the label when the dataset was processed. \n",
    "  In multi_class this is np.unique(class_series) and in multi-label np.unique(np.concatenate(class_series)).\n",
    "  Author: Angel Igareta (angel@igareta.com)\n",
    "  \"\"\"\n",
    "  if multi_class:\n",
    "    # If class is one hot encoded, transform to categorical labels to use compute_class_weight   \n",
    "    if one_hot_encoded:\n",
    "      class_series = np.argmax(class_series, axis=1)\n",
    "  \n",
    "    # Compute class weights with sklearn method\n",
    "    class_labels = np.unique(class_series)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=class_labels, y=class_series)\n",
    "    return dict(zip(class_labels, class_weights))\n",
    "  else:\n",
    "    # It is neccessary that the multi-label values are one-hot encoded\n",
    "    mlb = None\n",
    "    if not one_hot_encoded:\n",
    "      mlb = MultiLabelBinarizer()\n",
    "      class_series = mlb.fit_transform(class_series)\n",
    "\n",
    "    n_samples = len(class_series)\n",
    "    n_classes = len(class_series[0])\n",
    "\n",
    "    # Count each class frequency\n",
    "    class_count = [0] * n_classes\n",
    "    for classes in class_series:\n",
    "        for index in range(n_classes):\n",
    "            if classes[index] != 0:\n",
    "                class_count[index] += 1\n",
    "    \n",
    "    # Compute class weights using balanced method\n",
    "    class_weights = [n_samples / (n_classes * freq) if freq > 0 else 1 for freq in class_count]\n",
    "    class_labels = range(len(class_weights)) if mlb is None else mlb.classes_\n",
    "    return dict(zip(class_labels, class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:30:51.314225100Z",
     "start_time": "2024-05-01T12:30:51.308208200Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_class_weights(df,feature_names, alpha=10, beta=2):\n",
    "    class_weights = {}\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    # N = len(df)\n",
    "    n_features = len(feature_names)\n",
    "\n",
    "    N = np.sum(df[feature_names].to_numpy())\n",
    "    for label in feature_names:\n",
    "        if label in df.columns:\n",
    "            positive_weights[label] = np.log((N+n_features) /(alpha * (sum(df[label] == 1))+1)*beta)\n",
    "            negative_weights[label] = np.log((N+n_features) /(alpha * (sum(df[label] == 0))+1)*beta)\n",
    "        else:\n",
    "            positive_weights[label] = 0\n",
    "            negative_weights[label] = 0\n",
    "\n",
    "\n",
    "            \n",
    "    # class_weights['positive_weights'] = pd.DataFrame.from_dict(positive_weights)\n",
    "    # class_weights['negative_weights'] = pd.DataFrame.from_dict(negative_weights)\n",
    "    class_weights = pd.DataFrame(zip(positive_weights.keys(),positive_weights.values(), negative_weights.values()),columns=['class','positive_weights','negative_weights'])        \n",
    "    class_weights['method'] = 'inv'\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "data": {
      "text/plain": "array([15, 12, 12, ...,  9,  9,  9], dtype=int64)"
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw['label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T12:38:15.573459400Z",
     "start_time": "2024-05-01T12:38:15.563451600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T12:54:15.132723600Z",
     "start_time": "2024-05-01T12:54:15.118852800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    class  positive_weights\n5       1          3.605882\n4       2          3.004902\n7       3          0.824202\n15      4         16.026144\n9       5          0.200327\n13      6          6.868347\n8       7          1.428072\n2       8          0.579258\n3       9          0.199220\n16     10         13.112299\n14     11         10.302521\n1      12          0.863684\n12     13          3.517934\n6      14          2.289449\n0      15         13.112299\n10     16          7.591331\n11     17          3.698341",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>positive_weights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>3.605882</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>3.004902</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>0.824202</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>4</td>\n      <td>16.026144</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5</td>\n      <td>0.200327</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6</td>\n      <td>6.868347</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7</td>\n      <td>1.428072</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>0.579258</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0.199220</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10</td>\n      <td>13.112299</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>11</td>\n      <td>10.302521</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>0.863684</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>3.517934</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>14</td>\n      <td>2.289449</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>13.112299</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>7.591331</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>17</td>\n      <td>3.698341</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "df = df_train_raw\n",
    "labels = df['label'].values\n",
    "classes = df['label'].unique()\n",
    "class_weights_vals = compute_class_weight(class_weight=\"balanced\", classes=classes, y=labels)\n",
    "# compute_class_weight()\n",
    "class_weights = pd.DataFrame(zip(classes, class_weights_vals), columns=['class', 'positive_weights'])\n",
    "class_weights.sort_values(by='class', inplace=True)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T11:19:33.986694200Z"
    }
   },
   "outputs": [],
   "source": [
    "# class_series = df[feature_names].values\n",
    "\n",
    "# weights_dict_nums = generate_class_weights(class_series, multi_class=False, one_hot_encoded=True)\n",
    "# weights_dict_labels = pd.DataFrame({t:v for t, v in zip(feature_names, weights_dict_nums.values())})\n",
    "# weights_dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [
    {
     "data": {
      "text/plain": "'D:\\\\Project-mpg microgesture\\\\human_micro_gesture_classifier\\\\miga'"
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T12:59:40.535291800Z",
     "start_time": "2024-05-01T12:59:40.523294600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "'D:\\\\Project-mpg microgesture\\\\human_micro_gesture_classifier\\\\scripts\\\\miga_smg\\\\videomae_vit_base_patch16_224_kinetic_400_densepose_dual\\\\dataset'"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_folder"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-01T13:01:21.518127800Z",
     "start_time": "2024-05-01T13:01:21.510123300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T13:01:15.386308600Z",
     "start_time": "2024-05-01T13:01:15.351306800Z"
    }
   },
   "outputs": [],
   "source": [
    "save_folder = r'D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\scripts\\miga_smg\\videomae_vit_base_patch16_224_kinetic_400_densepose_dual\\dataset'\n",
    "# save_folder = osp.join(*save_folder.split('/'))\n",
    "\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "df_train.to_csv(osp.join(save_folder, 'train.csv'))\n",
    "# df_test.to_csv(osp.join(save_folder, 'test.csv'))\n",
    "df_val.to_csv(osp.join(save_folder, 'val.csv'))\n",
    "class_weights.to_csv(osp.join(save_folder, 'weights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-01T13:01:46.534773900Z",
     "start_time": "2024-05-01T13:01:46.514782700Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "wrapped_data = {'data': class_weights.to_dict(orient='records')}\n",
    "\n",
    "# Save wrapped data as JSON\n",
    "with open(osp.join(save_folder, 'weights.json'), 'w') as json_file:\n",
    "    json.dump(wrapped_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T11:19:33.989694300Z"
    }
   },
   "outputs": [],
   "source": [
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T11:19:33.991694600Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
