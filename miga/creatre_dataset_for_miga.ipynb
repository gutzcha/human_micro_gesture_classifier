{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:21.770506300Z",
     "start_time": "2024-05-11T23:50:19.644503800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import sys\n",
    "from const import ID2LABELS_SMG as LABEL2ID\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.insert(0, '..')\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:21.786518600Z",
     "start_time": "2024-05-11T23:50:21.772508500Z"
    }
   },
   "outputs": [],
   "source": [
    "path_to_root_folder = '\\Project-mpg microgesture\\smg\\smg_split_files'\n",
    "path_to_root_folder = osp.join(*path_to_root_folder.split('/'))\n",
    "\n",
    "train_data_path = osp.join(path_to_root_folder,'train.csv')\n",
    "val_data_path = osp.join(path_to_root_folder,'validation.csv')\n",
    "\n",
    "train_files_folder = osp.join(path_to_root_folder, 'train')\n",
    "val_files_folder = osp.join(path_to_root_folder, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:22.753747900Z",
     "start_time": "2024-05-11T23:50:22.695749300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_raw = pd.read_csv(train_data_path, index_col=0)\n",
    "df_val_raw = pd.read_csv(val_data_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:23.464782200Z",
     "start_time": "2024-05-11T23:50:23.439748300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   label  start_frame  end_frame  \\\n0     15          100        176   \n1     12          837        889   \n2     12         1710       1749   \n3      8         4849       4885   \n4      9         6330       6368   \n\n                                               paths    basename  durations  \n0  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         76  \n1  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         52  \n2  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         39  \n3  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         36  \n4  ..\\..\\smg\\smg_split_files\\train\\Sample0001_col...  Sample0001         38  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>start_frame</th>\n      <th>end_frame</th>\n      <th>paths</th>\n      <th>basename</th>\n      <th>durations</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15</td>\n      <td>100</td>\n      <td>176</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12</td>\n      <td>837</td>\n      <td>889</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>52</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>12</td>\n      <td>1710</td>\n      <td>1749</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8</td>\n      <td>4849</td>\n      <td>4885</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>6330</td>\n      <td>6368</td>\n      <td>..\\..\\smg\\smg_split_files\\train\\Sample0001_col...</td>\n      <td>Sample0001</td>\n      <td>38</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['label', 'start_frame', 'end_frame', 'paths', 'basename', 'durations'], dtype='object')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:35.677961400Z",
     "start_time": "2024-05-11T23:50:35.653956400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:36.229234800Z",
     "start_time": "2024-05-11T23:50:36.220478400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Structure of the final dataset: filename, labels\n",
    "# def get_video_list(files_folder):\n",
    "#     all_files = glob(osp.join(files_folder, '*.mp4'))\n",
    "#     base_names = [osp.basename(f) for f in all_files]\n",
    "#     sample_id = [int(osp.splitext(f)[0].split('-')[0]) for f in base_names]\n",
    "#     view = [(osp.splitext(f)[0].split('-video')[-1]) for f in base_names]\n",
    "#     df = pd.DataFrame(zip(all_files, base_names, sample_id, view), columns=['filenames', 'base_name', 'sample_id', 'view'])\n",
    "#     return df\n",
    "# df_train_files = get_video_list(files_folder=train_files_folder)\n",
    "# df_val_files = get_video_list(files_folder=val_files_folder)\n",
    "# # file_sample_ids.sort()\n",
    "# # file_sample_ids\n",
    "# # df_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<seaborn.axisgrid.FacetGrid at 0x182fcbb0ca0>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 500x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check if there is a correlation between the labels and the lengths\n",
    "df = df_train_raw.copy()\n",
    "import seaborn as sns\n",
    "sns.catplot(data=df, x='label', y='durations', kind='bar')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T01:22:10.480952Z",
     "start_time": "2024-05-12T01:22:09.081259800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def convert_df(df_in):\n",
    "        \n",
    "    df_out = pd.DataFrame()\n",
    "    \n",
    "    df_out['filenames'] = df_in['paths'].apply(lambda x: osp.basename(x))\n",
    "    df_out['folder_name'] = df_in['paths'].apply(lambda x: x.split('\\\\')[-2])\n",
    "    df_out['durations'] = df_in['durations']\n",
    "    \n",
    "    df_out['view'] = 'center'\n",
    "    metadata = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    metadata['sample_id'] = df_out['filenames'].apply(lambda x: x.split('.')[0].replace('Sample','').replace('color_',''))\n",
    "    metadata['start_frame'] = df_in['start_frame']\n",
    "    metadata['end_frame'] = df_in['end_frame']\n",
    "    metadata['label_txt'] = df_in['label'].apply(lambda x: LABEL2ID[x])\n",
    "    metadata['label']= df_in['label']\n",
    "    metadata['view'] =  df_out['view']\n",
    "    metadata['filenames'] = df_out['filenames']\n",
    "    metadata['durations'] = df_out['durations']\n",
    "    cat = df_in['label']\n",
    "    categories = list(LABEL2ID.keys())\n",
    "    one_hot_encoded = pd.get_dummies(cat.astype(pd.CategoricalDtype(categories=categories)))\n",
    "    \n",
    "    new_columns = one_hot_encoded.columns.to_list()\n",
    "    one_hot_encoded = one_hot_encoded.astype(float)\n",
    "    df_out['labels'] = one_hot_encoded.values.tolist()\n",
    "    # df_out['labels'] = df_in['label'] - 1\n",
    "    df_out['metadata'] = metadata.to_dict(orient='records')\n",
    "    \n",
    "    return df_out, new_columns, one_hot_encoded\n",
    "\n",
    "df_train,new_columns, train_one_hot_encoded = convert_df(df_train_raw)\n",
    "df_val, _, val_one_hot_encoded = convert_df(df_val_raw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:36.990244300Z",
     "start_time": "2024-05-11T23:50:36.950236300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0      {'sample_id': '0031_0001', 'start_frame': 940,...\n1      {'sample_id': '0031_0002', 'start_frame': 1245...\n2      {'sample_id': '0031_0003', 'start_frame': 1482...\n3      {'sample_id': '0031_0004', 'start_frame': 1744...\n4      {'sample_id': '0031_0005', 'start_frame': 1865...\n                             ...                        \n99     {'sample_id': '0035_0100', 'start_frame': 2361...\n100    {'sample_id': '0035_0101', 'start_frame': 2399...\n101    {'sample_id': '0035_0102', 'start_frame': 2436...\n102    {'sample_id': '0035_0103', 'start_frame': 2447...\n103    {'sample_id': '0035_0104', 'start_frame': 2457...\nName: metadata, Length: 637, dtype: object"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val['metadata']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T00:23:23.204762Z",
     "start_time": "2024-05-12T00:23:23.184761700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(df_val.iloc[0]['labels'])\n",
    "print(len(df_val.iloc[0]['labels']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-11T23:50:38.111788400Z",
     "start_time": "2024-05-11T23:50:38.086799Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T11:29:24.451907700Z",
     "start_time": "2024-05-08T11:29:24.420909Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_folder = r'D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\scripts\\miga_smg'\n",
    "run_name = 'videomae_vit_base_patch16_224_kinetic_400_densepose_dual_resample'\n",
    "\n",
    "os.makedirs(osp.join(experiment_folder, run_name), exist_ok=True)\n",
    "os.makedirs(osp.join(experiment_folder, run_name,'dataset'), exist_ok=True)\n",
    "\n",
    "df_val.to_csv(osp.join(experiment_folder,'val.csv'))\n",
    "df_train.to_csv(osp.join(experiment_folder,'train.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T11:29:28.332012700Z",
     "start_time": "2024-05-08T11:29:26.276939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\miga\\..\\modeling_finetune.py:306: UserWarning: Overwriting vit_small_patch16_224 in registry with modeling_finetune.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_small_patch16_224(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\miga\\..\\modeling_finetune.py:315: UserWarning: Overwriting vit_base_patch16_224 in registry with modeling_finetune.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_224(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\miga\\..\\modeling_finetune.py:324: UserWarning: Overwriting vit_base_patch16_384 in registry with modeling_finetune.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_base_patch16_384(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\miga\\..\\modeling_finetune.py:333: UserWarning: Overwriting vit_large_patch16_224 in registry with modeling_finetune.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_224(pretrained=False, **kwargs):\n",
      "D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\miga\\..\\modeling_finetune.py:342: UserWarning: Overwriting vit_large_patch16_384 in registry with modeling_finetune.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  def vit_large_patch16_384(pretrained=False, **kwargs):\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import dyadic_communication\n",
    "from importlib import reload\n",
    "import debug_model\n",
    "import run_videomae_vis_v2\n",
    "\n",
    "reload(dyadic_communication)\n",
    "reload(debug_model)\n",
    "reload(run_videomae_vis_v2)\n",
    "\n",
    "from argparse import Namespace\n",
    "import mpigroup.const as const\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T10:36:33.174771600Z",
     "start_time": "2024-05-07T10:36:33.142790600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'front1': 'tr', 'front2': 'tl', 'right': 'bl', 'left': 'br', 'center': 'mm'}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(const)\n",
    "cropping_map = const.cropping_map\n",
    "cropping_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T10:36:35.275444600Z",
     "start_time": "2024-05-07T10:36:33.752450100Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Project-mpg microgesture\\\\human_micro_gesture_classifier\\\\scripts\\\\miga_smg\\\\val.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m args\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      8\u001B[0m args\u001B[38;5;241m.\u001B[39mtest_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mdyadic_communication\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDyadicvideoClsDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[43m          \u001B[49m\u001B[43manno_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manno_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m            \u001B[49m\u001B[43mclip_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_segment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_frames\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtest_num_segment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_num_segment\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtest_num_crop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_num_crop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_crop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_aspect_ratio\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcrop_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m            \u001B[49m\u001B[43mshort_side_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshort_side_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnew_height\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnew_width\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m            \u001B[49m\u001B[43mview_crop_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcropping_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcorner_crop_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_root\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_root\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m            \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Project-mpg microgesture\\human_micro_gesture_classifier\\miga\\..\\dyadic_communication.py:54\u001B[0m, in \u001B[0;36mDyadicvideoClsDataset.__init__\u001B[1;34m(self, anno_path, data_path, mode, clip_len, crop_size, short_side_size, new_height, new_width, keep_aspect_ratio, num_segment, num_crop, test_num_segment, test_num_crop, view_crop_mapping, corner_crop_size, data_root, limit_data, args, **kwargs)\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to import `decord` which is required to read videos.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39manno_path, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m---> 54\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manno_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     56\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39manno_path\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\micro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\micro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\micro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\micro\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\micro\\lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'D:\\\\Project-mpg microgesture\\\\human_micro_gesture_classifier\\\\scripts\\\\miga_smg\\\\val.csv'"
     ]
    }
   ],
   "source": [
    "data_path= r'D:\\Project-mpg microgesture\\smg\\smg_split_files'\n",
    "\n",
    "args = debug_model.args\n",
    "args.anno_path = osp.join(experiment_folder,'val.csv')\n",
    "args.data_path = data_path\n",
    "args.data_root = data_path\n",
    "args.mode = 'validation'\n",
    "args.test_mode = False\n",
    "dataset = dyadic_communication.DyadicvideoClsDataset(\n",
    "          anno_path=args.anno_path,\n",
    "            data_path=args.data_path,\n",
    "            mode=args.mode,\n",
    "            clip_len=1,\n",
    "            num_segment=args.num_frames,\n",
    "            test_num_segment=args.test_num_segment,\n",
    "            test_num_crop=args.test_num_crop,\n",
    "            num_crop=1 if not args.test_mode else 3,\n",
    "            keep_aspect_ratio=True,\n",
    "            crop_size=args.input_size,\n",
    "            short_side_size=args.short_side_size,\n",
    "            new_height=224,\n",
    "            new_width=224,\n",
    "            view_crop_mapping=cropping_map,\n",
    "            corner_crop_size=None,\n",
    "            data_root=args.data_root,\n",
    "            args=args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:36.008172700Z",
     "start_time": "2024-05-06T12:58:35.994171300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:37.324251600Z",
     "start_time": "2024-05-06T12:58:37.317251500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = DataLoader(dataset=dataset, batch_size=5, shuffle=True)\n",
    "iterdata = iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:40.858344500Z",
     "start_time": "2024-05-06T12:58:37.814344100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 16, 224, 224])\n",
      "tensor([6, 6, 6, 6, 2])\n",
      "[('D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0082', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0031_color_0044', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0159', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0005', 'D:\\\\Project-mpg microgesture\\\\smg\\\\smg_split_files\\\\validation\\\\Sample0033_color_0113'), {'sample_id': ['0033_0082', '0031_0044', '0033_0159', '0033_0005', '0033_0113'], 'start_frame': tensor([ 6978, 11895, 12230,   766,  8978]), 'end_frame': tensor([ 7023, 12078, 12316,   776,  9020]), 'label_txt': ['Arms akimbo', 'Arms akimbo', 'Arms akimbo', 'Arms akimbo', 'Moving legs'], 'view': ['center', 'center', 'center', 'center', 'center'], 'filenames': ['Sample0033_color_0082.mp4', 'Sample0031_color_0044.mp4', 'Sample0033_color_0159.mp4', 'Sample0033_color_0005.mp4', 'Sample0033_color_0113.mp4'], 'durations': tensor([ 45, 183,  86,  10,  42])}, [], []]\n"
     ]
    }
   ],
   "source": [
    "d = next(iterdata)\n",
    "print(d[0].shape)\n",
    "print(d[1])\n",
    "print(d[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:42.529344700Z",
     "start_time": "2024-05-06T12:58:40.842352800Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(run_videomae_vis_v2)\n",
    "\n",
    "vids = d[0]\n",
    "labels = d[1]\n",
    "label_txt = d[3]['label_txt']\n",
    "fnames = d[3]['filenames']\n",
    "\n",
    "for vid,label, txt_label, fname in zip(vids,labels, label_txt, fnames):\n",
    "       \n",
    "    # ret = get_activities(feature_names, label)\n",
    "\n",
    "    txt =  '\\n'.join([fname ,txt_label])\n",
    "    \n",
    "    vid = run_videomae_vis_v2.unnormalize_frames(vid)\n",
    "    run_videomae_vis_v2.save_video(vid, osp.join('testing_'+fname), txt=txt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:42.545355900Z",
     "start_time": "2024-05-06T12:58:42.530347600Z"
    }
   },
   "outputs": [],
   "source": [
    "# video_path = osp.join(path_to_root_folder,'clips_val','05942-video1.mp4')\n",
    "# video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:42.571352400Z",
     "start_time": "2024-05-06T12:58:42.548346300Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'experiment':'MPIG_densepose_dual_2',\n",
    "    'description':'MPIG_densepose_dual - videoMAE-K400 , same as K400 but then was finetuned on MPIGroupInteractions dataset (train set) for 100 epochs, with denspose as additional decoding target',\n",
    "    'checkpoint_path':r'D:\\Project-mpg microgesture\\pretrained\\pretrained\\MPIIGroupInteraction\\k400_finetune_videomae_pretrain_dual_2_patch16_224_frame_16x4_tube_mask_ratio_0.9_e100\\checkpoint-99.pth',\n",
    "    'model_name':'pretrain_videomae_base_patch16_224_densepose_dual',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:42.580344200Z",
     "start_time": "2024-05-06T12:58:42.566348700Z"
    }
   },
   "outputs": [],
   "source": [
    "image_batch = d[0]\n",
    "save_folder = osp.join('videos')\n",
    "model_path = model_dict['checkpoint_path']\n",
    "model_name = model_dict['model_name']\n",
    "\n",
    "args = Namespace(\n",
    "        image_batch=image_batch,\n",
    "        save_path=save_folder, # list\n",
    "        model_path=model_path, \n",
    "        mask_type='tube',\n",
    "        num_frames=16,\n",
    "        sampling_rate=4,\n",
    "        decoder_depth=4,\n",
    "        input_size=224,\n",
    "        device='cuda:0',\n",
    "        imagenet_default_mean_and_std=True,\n",
    "        mask_ratio=0,\n",
    "        model=model_name,\n",
    "        densepose=True,\n",
    "        drop_path=0.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:58:47.064376800Z",
     "start_time": "2024-05-06T12:58:42.581345100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: pretrain_videomae_base_patch16_224_densepose_dual\n"
     ]
    },
    {
     "data": {
      "text/plain": "PretrainVisionTransformerMultiOutout(\n  (encoder): PretrainVisionTransformerEncoder(\n    (patch_embed): PatchEmbed(\n      (proj): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n    )\n    (blocks): ModuleList(\n      (0-11): 12 x Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=False)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU(approximate='none')\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (head): Identity()\n  )\n  (decoder): PretrainVisionTransformerDecoderMultiOutput(\n    (decoders): ModuleList(\n      (0-1): 2 x PretrainVisionTransformerDecoder(\n        (blocks): ModuleList(\n          (0-3): 4 x Block(\n            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n            (attn): Attention(\n              (qkv): Linear(in_features=384, out_features=1152, bias=False)\n              (attn_drop): Dropout(p=0.0, inplace=False)\n              (proj): Linear(in_features=384, out_features=384, bias=True)\n              (proj_drop): Dropout(p=0.0, inplace=False)\n            )\n            (drop_path): Identity()\n            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n            (mlp): Mlp(\n              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n              (act): GELU(approximate='none')\n              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n              (drop): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n        (head): Linear(in_features=384, out_features=1536, bias=True)\n      )\n    )\n  )\n  (encoder_to_decoder): Linear(in_features=768, out_features=384, bias=False)\n)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = run_videomae_vis_v2.get_model(args=args) \n",
    "\n",
    "checkpoint = torch.load(args.model_path, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "\n",
    "# outputs = model(image_batch)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:02.874805200Z",
     "start_time": "2024-05-06T12:58:47.067380300Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(image_batch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:03.276806300Z",
     "start_time": "2024-05-06T12:59:02.879806200Z"
    }
   },
   "outputs": [],
   "source": [
    "reload(run_videomae_vis_v2)\n",
    "\n",
    "\n",
    "\n",
    "rec_videos_patches = outputs[0]\n",
    "rec_densepose_patches = outputs[1]\n",
    "patch_size = model.encoder.patch_embed.patch_size\n",
    "unnorm_videos = run_videomae_vis_v2.unnormalize_frames(img=image_batch)\n",
    "_, rec_videos, _ = run_videomae_vis_v2.reconstruct_video_from_patches(\n",
    "    ori_img=unnorm_videos, patch_size=patch_size, bool_masked_pos=None, outputs=rec_videos_patches, frame_id_list=None)\n",
    "\n",
    "_, rec_densepose, _ = run_videomae_vis_v2.reconstruct_video_from_patches(\n",
    "    ori_img=unnorm_videos, patch_size=patch_size, bool_masked_pos=None, \n",
    "    outputs=rec_densepose_patches, frame_id_list=None,normalize_with_orig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 16, 224, 224])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_densepose[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:03.290804800Z",
     "start_time": "2024-05-06T12:59:03.272811900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# a = (torch.nn.functional.normalize(rec_densepose[0],dim=0) + torch.Tensor([1,1,1])[:,None,None,None]) / torch.Tensor([2,2,2])[:,None,None,None]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:03.332804800Z",
     "start_time": "2024-05-06T12:59:03.288806100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# print(f'mean: {a.mean(dim=[1,2,3])}')\n",
    "# print(f'std: {a.std(dim=[1,2,3])}')\n",
    "# print(f'min: {a.reshape(3, -1).min(dim=1)[0]}')\n",
    "# print(f'max: {a.reshape(3, -1).max(dim=1)[0] }')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:03.334803700Z",
     "start_time": "2024-05-06T12:59:03.303807100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:03.372806900Z",
     "start_time": "2024-05-06T12:59:03.321804900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([-1.6935e-05, -6.8499e-05,  3.8860e-05], grad_fn=<MeanBackward1>)\n",
      "std: tensor([0.1546, 0.1683, 0.1660], grad_fn=<StdBackward0>)\n",
      "min: tensor([-2.4869, -2.4794, -1.7007], grad_fn=<MinBackward0>)\n",
      "max: tensor([7.5820, 7.5551, 5.9062], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'mean: {rec_densepose[0].mean(dim=[1,2,3])}')\n",
    "print(f'std: {rec_densepose[0].std(dim=[1,2,3])}')\n",
    "print(f'min: {rec_densepose[0].reshape(3, -1).min(dim=1)[0]}')\n",
    "print(f'max: {rec_densepose[0].reshape(3, -1).max(dim=1)[0] }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:05.262844500Z",
     "start_time": "2024-05-06T12:59:03.371809Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "vids = d[0]\n",
    "labels = d[1]\n",
    "label_txt = d[3]['label_txt']\n",
    "fnames = d[3]['filenames']\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "for ori_vid,rec_vid,rec_dense,labels,txt_label, fname in zip(vids,rec_videos,rec_densepose,labels, label_txt, fnames):\n",
    "       \n",
    "    txt =  '\\n'.join([fname ,txt_label])\n",
    "\n",
    "    ori_vid = run_videomae_vis_v2.unnormalize_frames(ori_vid)\n",
    "    rec_dense = run_videomae_vis_v2.unnormalize_frames(rec_dense)\n",
    "    # rec_dense = (torch.nn.functional.normalize(rec_dense,dim=0) + torch.Tensor([1,1,1])[:,None,None,None]) / torch.Tensor([2,2,2])[:,None,None,None]\n",
    "    # rec_vid = run_videomae_vis_v2.unnormalize_frames(rec_vid)\n",
    "    # rec_dense = softmax(rec_dense)\n",
    "\n",
    "    save_folder = osp.join('videos',fname.replace('.mp4',''))\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    \n",
    "    run_videomae_vis_v2.save_video(ori_vid, osp.join(save_folder,'ori_vid_'+fname), txt=txt)\n",
    "\n",
    "    # run_videomae_vis_v2.save_video(ori_dense, osp.join(save_folder,'ori_dense_'+fname), txt=txt)\n",
    "\n",
    "    run_videomae_vis_v2.save_video(rec_vid, osp.join(save_folder,'rec_vid_'+fname), txt=txt)\n",
    "\n",
    "    run_videomae_vis_v2.save_video(rec_dense, osp.join(save_folder,'rec_dense_'+fname), txt=txt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T12:59:05.640871400Z",
     "start_time": "2024-05-06T12:59:05.266805Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T11:52:54.541167Z",
     "start_time": "2024-05-08T11:52:54.522161Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_class_weights(df,feature_names, alpha=10, beta=2):\n",
    "    class_weights = {}\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    class_frequency = {}\n",
    "    # N = len(df)\n",
    "    n_features = len(feature_names)\n",
    "\n",
    "    N = np.sum(df[feature_names].to_numpy())\n",
    "    for label in feature_names:\n",
    "        if label in df.columns:\n",
    "            positive_weights[label] = np.log((N+n_features) /(alpha * (sum(df[label] == 1))+1)*beta)\n",
    "            negative_weights[label] = np.log((N+n_features) /(alpha * (sum(df[label] == 0))+1)*beta)\n",
    "            \n",
    "        else:\n",
    "            positive_weights[label] = 0\n",
    "            negative_weights[label] = 0\n",
    "        \n",
    "        class_frequency[label] =  sum(df[label] == 1) / N\n",
    "\n",
    "            \n",
    "    # class_weights['positive_weights'] = pd.DataFrame.from_dict(positive_weights)\n",
    "    # class_weights['negative_weights'] = pd.DataFrame.from_dict(negative_weights)\n",
    "    class_weights = pd.DataFrame(zip(positive_weights.keys(),positive_weights.values(), negative_weights.values(),class_frequency.values()),columns=['class','positive_weights','negative_weights', 'class_frequency'])        \n",
    "    class_weights['method'] = 'inv'\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "    class  positive_weights  negative_weights  class_frequency method\n0       1          3.015778         -1.075394         0.016313    inv\n1       2          2.834835         -1.072072         0.019576    inv\n2       3          1.546267         -1.017804         0.071370    inv\n3       4          4.479364         -1.088162         0.003670    inv\n4       5          0.133242         -0.744268         0.293638    inv\n5       6          3.652685         -1.083239         0.008564    inv\n6       7          2.094541         -1.049782         0.041191    inv\n7       8          1.194166         -0.984770         0.101550    inv\n8       9          0.127705         -0.741957         0.295269    inv\n9      10          4.285208         -1.087343         0.004486    inv\n10     11          4.050368         -1.086114         0.005710    inv\n11     12          1.592968         -1.021311         0.068108    inv\n12     13          2.991287         -1.074979         0.016721    inv\n13     14          2.564544         -1.065813         0.025693    inv\n14     15          4.285208         -1.087343         0.004486    inv\n15     16          3.751125         -1.084061         0.007749    inv\n16     17          3.040884         -1.075808         0.015905    inv",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>positive_weights</th>\n      <th>negative_weights</th>\n      <th>class_frequency</th>\n      <th>method</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3.015778</td>\n      <td>-1.075394</td>\n      <td>0.016313</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2.834835</td>\n      <td>-1.072072</td>\n      <td>0.019576</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.546267</td>\n      <td>-1.017804</td>\n      <td>0.071370</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>4.479364</td>\n      <td>-1.088162</td>\n      <td>0.003670</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.133242</td>\n      <td>-0.744268</td>\n      <td>0.293638</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>3.652685</td>\n      <td>-1.083239</td>\n      <td>0.008564</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>2.094541</td>\n      <td>-1.049782</td>\n      <td>0.041191</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1.194166</td>\n      <td>-0.984770</td>\n      <td>0.101550</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0.127705</td>\n      <td>-0.741957</td>\n      <td>0.295269</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>4.285208</td>\n      <td>-1.087343</td>\n      <td>0.004486</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>4.050368</td>\n      <td>-1.086114</td>\n      <td>0.005710</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>1.592968</td>\n      <td>-1.021311</td>\n      <td>0.068108</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>2.991287</td>\n      <td>-1.074979</td>\n      <td>0.016721</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>2.564544</td>\n      <td>-1.065813</td>\n      <td>0.025693</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>4.285208</td>\n      <td>-1.087343</td>\n      <td>0.004486</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>3.751125</td>\n      <td>-1.084061</td>\n      <td>0.007749</td>\n      <td>inv</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>3.040884</td>\n      <td>-1.075808</td>\n      <td>0.015905</td>\n      <td>inv</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_one_hot_encoded\n",
    "feature_names = LABEL2ID.keys()\n",
    "\n",
    "class_weights =  get_class_weights(df,feature_names, alpha=3, beta=1)\n",
    "class_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:52:55.135169800Z",
     "start_time": "2024-05-08T11:52:55.104160700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "array([15, 12, 12, ...,  9,  9,  9], dtype=int64)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw['label'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:53:11.856191200Z",
     "start_time": "2024-05-08T11:53:11.836184300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:53:15.728260600Z",
     "start_time": "2024-05-08T11:53:15.706254900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T11:53:16.298254100Z",
     "start_time": "2024-05-08T11:53:16.275268100Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# df = df_train_raw\n",
    "# labels = df['label'].values\n",
    "# classes = df['label'].unique()\n",
    "# class_weights_vals = compute_class_weight(class_weight=\"balanced\", classes=classes, y=labels)\n",
    "# # compute_class_weight()\n",
    "# class_weights = pd.DataFrame(zip(classes, class_weights_vals), columns=['class', 'positive_weights'])\n",
    "# class_weights.sort_values(by='class', inplace=True)\n",
    "# class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "                   filenames folder_name  durations    view  \\\n0  Sample0001_color_0001.mp4       train         76  center   \n1  Sample0001_color_0002.mp4       train         52  center   \n2  Sample0001_color_0003.mp4       train         39  center   \n3  Sample0001_color_0004.mp4       train         36  center   \n4  Sample0001_color_0005.mp4       train         38  center   \n\n                                              labels  \\\n0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...   \n4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n\n                                            metadata  \n0  {'sample_id': '0001_0001', 'start_frame': 100,...  \n1  {'sample_id': '0001_0002', 'start_frame': 837,...  \n2  {'sample_id': '0001_0003', 'start_frame': 1710...  \n3  {'sample_id': '0001_0004', 'start_frame': 4849...  \n4  {'sample_id': '0001_0005', 'start_frame': 6330...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filenames</th>\n      <th>folder_name</th>\n      <th>durations</th>\n      <th>view</th>\n      <th>labels</th>\n      <th>metadata</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sample0001_color_0001.mp4</td>\n      <td>train</td>\n      <td>76</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0001', 'start_frame': 100,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Sample0001_color_0002.mp4</td>\n      <td>train</td>\n      <td>52</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0002', 'start_frame': 837,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sample0001_color_0003.mp4</td>\n      <td>train</td>\n      <td>39</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0003', 'start_frame': 1710...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Sample0001_color_0004.mp4</td>\n      <td>train</td>\n      <td>36</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n      <td>{'sample_id': '0001_0004', 'start_frame': 4849...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Sample0001_color_0005.mp4</td>\n      <td>train</td>\n      <td>38</td>\n      <td>center</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...</td>\n      <td>{'sample_id': '0001_0005', 'start_frame': 6330...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:53:17.810320Z",
     "start_time": "2024-05-08T11:53:17.790312700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T11:53:32.968684400Z",
     "start_time": "2024-05-08T11:53:32.928692400Z"
    }
   },
   "outputs": [],
   "source": [
    "save_folder = r'D:\\Project-mpg microgesture\\human_micro_gesture_classifier\\scripts\\miga_smg\\videomae_vit_base_patch16_224_kinetic_400_densepose_dual_multi\\dataset'\n",
    "# save_folder = osp.join(*save_folder.split('/'))\n",
    "\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "df_train.to_csv(osp.join(save_folder, 'train.csv'))\n",
    "# df_test.to_csv(osp.join(save_folder, 'test.csv'))\n",
    "df_val.to_csv(osp.join(save_folder, 'val.csv'))\n",
    "class_weights.to_csv(osp.join(save_folder, 'weights.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T13:24:14.552613600Z",
     "start_time": "2024-05-06T13:24:14.531607400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "wrapped_data = {'data': class_weights.to_dict(orient='records')}\n",
    "\n",
    "# Save wrapped data as JSON\n",
    "with open(osp.join(save_folder, 'weights.json'), 'w') as json_file:\n",
    "    json.dump(wrapped_data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T11:19:33.989694300Z"
    }
   },
   "outputs": [],
   "source": [
    "len(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-01T11:19:33.991694600Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
